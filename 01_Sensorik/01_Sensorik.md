<!--

author:   Sebastian Zug & Georg J√§ger & Copilot
email:    sebastian.zug@informatik.tu-freiberg.de & Georg.Jaeger@informatik.tu-freiberg.de
version:  0.2.0
language: de
narrator: Deutsch Female

import: https://github.com/liascript/CodeRunner
        https://raw.githubusercontent.com/liascript-templates/plantUML/master/README.md
        https://raw.githubusercontent.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/refs/heads/master/config.md

-->

[![LiaScript](https://raw.githubusercontent.com/LiaScript/LiaScript/master/badges/course.svg)](https://liascript.github.io/course/?https://raw.githubusercontent.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/refs/heads/master/01_Sensorik/01_Sensorik.md)

# Sensorik mobiler Roboter

| Parameter            | Kursinformationen                                                                                       |
| -------------------- | ------------------------------------------------------------------------------------------------------- |
| **Veranstaltung:**   | @config.lecture                                                                                         |
| **Semester**         | @config.semester                                                                                        |
| **Hochschule:**      | `Technische Universit√§t Freiberg`                                                                       |
| **Inhalte:**         | `Sensorsysteme f√ºr mobile Roboter`                                                                      |
| **Link auf GitHub:** | https://github.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/blob/master/01_Sensorik/01_Sensorik.md |
| **Autoren**          | @author                                                                                                 |

![](https://media.giphy.com/media/md1gcbifqcZUs/giphy.gif)

--------------------------------------------------------------------------------

**Zielstellung der heutigen Veranstaltung**

+ Verst√§ndnis der Rolle von Sensoren in der Robotik-Architektur
+ Klassifikation von Sensorsystemen nach verschiedenen Kriterien
+ Kennenlernen propriozeptiver Sensoren (Odometrie, IMU, Gyro, Kompass)
+ √úberblick √ºber exterozeptive Sensoren (GPS, Ultraschall)
+ Verst√§ndnis grundlegender Messprinzipien und Fehlerquellen

--------------------------------------------------------------------------------

## Einordnung: Sensoren in der Roboterarchitektur

    --{{0}}--
Willkommen zur zweiten Vorlesung! In VL 1 haben wir die Systemarchitektur eines Roboters kennengelernt ‚Äì das Sense-Think-Act-Paradigma. Heute fokussieren wir uns auf die unterste Ebene: die Sensorik. Sensoren sind die Augen, Ohren und das Tastgef√ºhl des Roboters.

Sensor in der Architekturansicht
====================

```ascii
                    Statusmeldungen 
     Nutzereingaben  ^                                       
                 |   |
Befehle          v   |
            +-----------------------+
            | Handlungsplanung      |  "$Strategie$"
            +-----------------------+
                 |   ^      | | |       Folge von Aktionen     
                 v   |      v v v
            +-----------------------+
            | Ausf√ºhrung            |  "$Taktik$"           
            +-----------------------+
                     ^      | | |       Geplante Trajektorie,
Status               |      v v v       Verhalten
            +-----------------------+
            | Reaktive √úberwachung  |  "$Ausf√ºhrung$"
            +-----------------------+
Sensordaten-    ^ ^ ^       | | |       Steuerbefehle an den 
erfassung       | | |       v v v       Aktuator 
            +----------+ +----------+
            | Sensoren | | Aktoren  |                               
            +----------+ +----------+
                  ^           |
                  |           v      
            .-----------------------.
            | Umgebung              |
            .-----------------------.                                                                                     .
```

    --{{1}}--
Die Verarbeitungskette zeigt: Sensordaten werden gefiltert, bevor sie f√ºr Regelung genutzt werden. Diese Filterung behandeln wir in VL 9-10. Heute: die Sensoren selbst.

                  {{1-2}}
*******************************************************************************

Sensor-Verarbeitungskette
====================


```ascii

       +----------+        +----------+                                
     +-+--------+ |     +--+--------+ |     +----------+     +----------+
+--> |Sensoren  +-+ --> | Filterung +-+ --> | Regelung | --> | Aktoren  | ---+
|    +----------+       +-----------+       +----------+     +----------+    |
|                                                                            |
|                              .---------------------.                       |
+----------------------------- | Umgebung            | <---------------------+
                               .---------------------.                                                                    .
```

*******************************************************************************

## Was ist ein Sensor?

    --{{0}}--
Ein Sensor transformiert physikalische, chemische oder biologische Messgr√∂√üen in elektrische Signale. Er ist das Interface zwischen dem technischen System und der realen Welt.

> **Definition:** Sensoren (lat. ‚Äûf√ºhlen") transformieren physikalische, chemische oder biologische Messgr√∂√üen in elektrische Signale.

![Fliehkraftregler](images/Fliehkrafregler.png "Historisches Beispiel: Fliehkraftregler Wikipedia Commons, Nutzer: Kino")<!-- width="50%" -->

*Fliehkraftregler ‚Äì ein mechanischer "Sensor", der Drehzahl in eine mechanische Stellgr√∂√üe umwandelt.*


## Klassifikation von Sensoren

    --{{0}}--
Sensoren lassen sich nach verschiedenen Kriterien klassifizieren. Diese Unterscheidungen helfen bei der Auswahl f√ºr spezifische Aufgaben.

**Nach Bezug zum Messobjekt:**

+ **Intern (propriozeptiv)**: Messen den Roboterzustand
  - Radencoder, IMU, Motorstr√∂me
+ **Extern (exterozeptiv)**: Messen die Umgebung
  - Kamera, Lidar, Ultraschall

**Nach Umgebungsbeeinflussung:**

+ **Aktiv**: Senden Signale aus (Ultraschall, Lidar)
+ **Passiv**: Nur Empf√§nger (Kamera, Mikrofon)

**Nach Dimension:**

+ 1D (Distanz), 2D (Bild), 2.5D (Tiefenbild), 3D (Punktwolke)

**Nach Art der Messung:**

<!--
style="width: 80%; max-width: 720px;"
-->
```ascii

    Sensoren f√ºr Lage und Position
                   |
      +------------+--------------+
      |                           |
 interne Sensoren         externe Sensoren
 (propriozeptiv)          (exterozeptiv)
      |                           |
  Odometrie              +--------+----------+
  IMU                    |                   |
  Gyro               taktil            ber√ºhrungslos
  Kompass                                    |
                          +------------+-----+-------+---------+
                          |            |             |         |
                      akustisch     Laser      bildbasiert   . . . .             .
```

### Integrationsebenen

    --{{0}}--
Moderne Sensoren bestehen oft aus mehreren Verarbeitungsstufen: vom Messelement √ºber Signalaufbereitung bis zum intelligenten Sensorsystem.

![Sensorintegration](images/SensorIntegrationsLevel.png "Integrationsebenen von Sensoren")<!-- width="80%" -->

### Wichtige Sensorparameter

| Parameter            | Bedeutung                                                                     | Beispielangabe f√ºr Ultraschallsensor                  |
| -------------------- | ----------------------------------------------------------------------------- | ------------------------- |
| **Messbereich**      | Ausdehnung der erfassbaren Messgr√∂√üe                                          | 0-400cm    |
| **Aufl√∂sung**        | Kleinste unterscheidbare √Ñnderung                                             | 1mm, 0.1¬∞                 |
| **Linearit√§t**       | Zusammenhang Eingangsgr√∂√üe ‚Üî Ausgabe                                         | $y=mx+n$ hier $d=\frac{1}{2}v\cdot \Delta t$                 |
| **Messfrequenz**     | H√§ufigkeit der Abtastung                                                      | 10Hz, 100Hz               |
| **Querempfindlichkeit** | Abh√§ngigkeit von anderen Parametern                                       | Temperatur bei Ultraschall|
| **√ñffnungswinkel**   | Erfassungsbereich bei gerichteten Sensoren                                    | 15¬∞, 60¬∞                  |

![Ultraschall](images/beam_srf235.gif "[^3]")<!-- width="40%" -->
![Ultraschall](images/beam_srf4.gif "[^3]")<!-- width="40%" -->

*Abstrahlkeulen zweier Ultraschallsensoren ‚Äì unterschiedliche √ñffnungswinkel!*

[^3]: [robot electronics faq](https://www.robot-electronics.co.uk/htm/sonar_faq.htm)


## Propriozeptive Sensorik 

    --{{0}}--
Propriozeptive Sensoren messen den inneren Zustand des Roboters (z. B. Pose, Orientierung, Geschwindigkeit) und sind unabh√§ngig von Umgebungsbedingungen. Sie liefern hohe Abtastraten, sind jedoch ohne √§u√üere Referenz driftanf√§llig (Integrationsfehler) und liefern keine direkte Information √ºber die Umgebung.

> **Definition:** Propriozeptive Sensoren erfassen ausschlie√ülich interne Zustandsgr√∂√üen eines Roboters (z. B. Geschwindigkeit $\mathbf{v}$, Drehrate $\boldsymbol\omega$, Str√∂me, Encoderimpulse) ohne externe Referenz. Sie sind robust gegen√ºber Umgebungsbedingungen, liefern jedoch keine direkten Informationen √ºber die Umwelt und m√ºssen zur Langzeitstabilit√§t mit externen Referenzen fusioniert werden.

    {{1}}
******************************************************

Unter der Pose versteht man die Kombination aus Position $\mathbf{p} = [x, y, z]^\top$ und Orientierung (z. B. als Rotationsmatrix $R \in SO(3)$ oder Quaternion $\mathbf{q}$). Die sechs Freiheitsgrade setzen sich aus drei Translationen $(x,y,z)$ und drei Rotationen zusammen: Roll $\phi$ (um x), Pitch $\theta$ (um y), Yaw $\psi$ (um z). 

> **Hinweis:** Die Pose ist ein zentraler Aspekt der Robotik, da sie die vollst√§ndige Beschreibung der Position und Orientierung eines Roboters im Raum erm√∂glicht. Die Grundlagen dazu werden Thema der Vorlesung n√§chste Woche sein.

********************************************************

    --{{1}}--
Bezug zur IMU: Gyroskope messen die Drehrate $\boldsymbol\omega_B$, Beschleunigungssensoren die spezifische Kraft $\mathbf{f}_B \approx \mathbf{a}_B - \mathbf{g}_B$, Magnetometer liefern eine absolute Kursreferenz (Yaw) relativ zum Erdmagnetfeld. Euler-Winkel sind anschaulich, aber anf√§llig f√ºr Singularit√§ten (Gimbal-Lock); f√ºr Fusion/Regelung werden oft Quaternionen bevorzugt.

### Inertialsensorik (IMU)

    --{{0}}--
Ein Tr√§gheitsnavigationssystem misst Bewegungen √ºber sechs Freiheitsgrade. Moderne IMUs kombinieren mehrere Sensoren in einem Chip.

    {{0-1}}
******************************************************

**Gemessene Gr√∂√üen:**

+ **Beschleunigung** (3 Achsen): $a_x, a_y, a_z$
+ **Winkelgeschwindigkeit** (3 Achsen): $\omega_x, \omega_y, \omega_z$
+ **Magnetfeld** (3 Achsen): $B_x, B_y, B_z$
+ (optional) Temperatur f√ºr die Kompensation

********************************************************

    --{{1}}--
Aus den Beschleunigungsdaten l√§sst sich die Geschwindigkeit berechnen: $v=v_0 +\sum a_i\cdot \Delta t_i$. Allerdings f√ºhrt die Integration zu Drift-Fehlern!

    {{1-2}}
******************************************************

Beschleunigungssensoren
================================

**Funktionsprinzip (MEMS):**

Mikro-elektro-mechanische Systeme (MEMS) sind Feder-Masse-Systeme, bei denen die ‚ÄûFedern‚Äú nur wenige Œºm breite Silicium-Stege sind und auch die Masse aus Silicium hergestellt ist. Durch die Auslenkung bei Beschleunigung kann zwischen dem gefedert aufgeh√§ngten Teil und einer festen Bezugselektrode eine √Ñnderung der elektrischen Kapazit√§t gemessen werden.

!?[](https://www.youtube.com/watch?v=swCTbz5sIQM "Visualisierung der Funktionsweise eines MEMS-Beschleunigungssensors")

**Beispiel: Fahrstuhlfahrt**

![Beschleunigung](images/BeschleunigungsSensor.png "[^Kling]")<!-- width="30%" -->
![Geschwindigkeit](images/BeschleunigungsSensorV.png "[^Kling]")<!-- width="30%" -->
![Position](images/BeschleunigungsSensorS.png "[^Kling]")<!-- width="30%" -->

*Aufzeichnung einer Fahrstuhlfahrt: Beschleunigung ‚Üí Geschwindigkeit ‚Üí Position - Jordi Kling, [HU Berlin](https://blogs.hu-berlin.de/didaktikdigital/2016/11/20/zurckgelegter-weg-einer-fahrstuhlfahrt-mit-handysensorik/)*
 

> **Problem: Drift!** ‚Üí L√∂sung: Kombination mit GPS (VL 9-10: Sensorfusion)

******************************************************


    {{2-3}}
******************************************************

Gyroskope (Drehratensensoren)
================================

> Gyroskope messen Rotationsgeschwindigkeiten um die drei Raumachsen. Durch Integration ergeben sich Orientierungswinkel.

**Gemessene Gr√∂√üen:**

+ **Gierrate** (yaw): Drehung um Hochachse
+ **Nickrate** (pitch): Drehung um Querachse
+ **Rollrate** (roll): Drehung um L√§ngsachse

!?[](https://www.youtube.com/watch?v=REVp33SwwHE)

******************************************************

    --{{3}}--
Magnetfeldsensoren messen das Erdmagnetfeld zur Orientierungsbestimmung. Achtung: St√∂rungen durch Metallstrukturen!
Insbesondere in Innenr√§umen ist das Magnetfeld stark gest√∂rt durch Metallstrukturen, Elektronik und Stromkabel.

    {{3-5}}
******************************************************

Kompasssensoren (Magnetometer)
================================

![Erdmagnetfeld](https://upload.wikimedia.org/wikipedia/commons/3/31/Geomagnetismus.png " Erdkugel mit Dipol und verbogenen Feldlinien - Autor Hubi, https://commons.wikimedia.org/wiki/File:Geomagnetismus.png")<!-- width="60%" -->

**Messprinzipien:**

1. **Hall-Effekt**: Spannung ‚ä• zu Strom und Magnetfeld
2. **Magnetoresistiv**: Widerstand √§ndert sich mit Magnetfeld

![KMZ52](images/KMZ52.png "Magnetoresistiver Sensor KMZ52 [Honeywell]")<!-- width="50%" -->

> Achtung: Insbesondere bei Innenraumanwendungen unterliegen Kompasse starken St√∂rungen.

![RoboterSystem](images/KompassStoerungen.png "*Klassen von St√∂rungen f√ºr Kompasssensoren* [Philips Electronic Compass Designusing KMZ51 and KMZ52](https://pdfs.semanticscholar.org/ad20/e5c06b4524fdef0f1dee5b83641822abd609.pdf) ")

******************************************************

    {{4-5}}
******************************************************

Diese kann ich aber wiederum nutzen, um anhand von Referenzkarten Positionen zu bestimmen. 

![RoboterSystem](images/MagnetfeldRoboter.png "*Robotersystem mit einem Array von Magnetfeldsensoren zur Datenerfassung* [Dissertation Filip Filipow]")<!-- width="60%" -->
![RoboterSystem](images/LokalisierungMagnetfeld.png "*Fl√§chige Aufnahme der Richtungsinformationen des Magnetfeldes* [Dissertation Filip Filipow]")<!-- width="60%" -->

******************************************************************

### Odometrie

    --{{0}}--
Odometrie erfasst die Bewegung durch Messung der Radumdrehungen. Kernkomponente: der Encoder.

**Prinzipien:**

+ Photoelektrisch (Gabellichtschranke)
+ Magnetisch (Hall-Sensoren)
+ Induktiv

![Encoder](images/Gabellichtschranke.png "Encoder mit Gabellichtschranke [Wikipedia, Autor Tycho]")<!-- width="30%" -->

**Konzepte**

+ __Inkrementelle Kodierung:__ zur Bestimmung der relativen Lage/Drehgeschwindigkeit anhand einer Impulsfolge,
Abw√§gung der Impulszahl pro Drehung von der
+ __Absolute Kodierung:__  Lageermittlung gegen√ºber einem Fixpunkt, aufwendige Drehimpulsgeber bis hunderttausenden Impulsen pro Umdrehung, h√§ufigste Codierung: Gray-Code (nur auf einem Ausgangssignal findet eine Signal√§nderung)

**Zwei-Kanal-Encoder:**

    --{{0}}--
Zwei um 90¬∞ phasenverschobene Signale erm√∂glichen Drehrichtungserkennung und Impulsverdopplung.

> Welchen Vorteil hat ein Zwei-Kanal-Encoder gegen√ºber einem Ein-Kanal-Encoder?

<!--
style="width: 80%; max-width: 700px;"
-->
```ascii

          ^           90¬∞
          |         |<--->|
          |      +-----+     +------
Kanal A   |      |     |     |
          | -----+     +-----+
          |
          | --+     +-----+     +----
Kanal B   |   |     |     |     |
          |   +-----+     +-----+
          |                                   
Drehrich- | --> Vorw√§rts (A f√ºhrt B)
tung      | <-- R√ºckw√§rts (B f√ºhrt A)
          +--------------------------->       
```

## Exterozeptive Sensoren (Distanzen und Positionen)

    --{{0}}--
Nun zu den Sensoren, die die Umgebung wahrnehmen. Wir konzentrieren uns auf Entfernungsmessung und Positionsbestimmung.

Entfernungsmessung: Grundprinzipien

**Vier Verfahren:**

1. **Amplitudenbasiert**: Signalst√§rke ‚Üí Entfernung
2. **Laufzeitbasiert**: Œît zwischen Senden/Empfangen
3. **Phasenbasiert**: Phasenverschiebung
4. **Triangulation**: Geometrische Berechnung

### Laufzeitmessung (Ultraschall)

    --{{0}}--
Ultraschallsensoren senden einen Schallimpuls aus und messen die Laufzeit des Echos. Entfernung: $d = \frac{1}{2} c \cdot t$

Das Laufzeitverfahren basiert auf der Messung des Zeitversatzes zwischen dem Aussenden eines Impulses und dem Empfang von dessen Reflexion.

<!--
style="width: 80%; max-width: 700px; display: block; margin-left: auto; margin-right: auto;"
-->
```ascii

          ^       
          |      +-----+   
          |      |     |
Sender    |      |     |     
          | -----+     +-----+
          |     Sendeimpuls
          |              
          |      |<------------->| Œît Laufzeit der ersten Echoantwort
          |
          |                      .-----.     2. Echo
          |                      |     |   .---.
Empf√§nger |   | St√∂rungen   |    |     |   |   |
          | --+-------------+----.     .---.   .-----------
          +--------------------------------------------------->               .
                                  Zeit      
```

Prominentestes Beispiel f√ºr Laufzeitsensoren sind ultraschallbasierte Systeme.

Aussenden eines Schallimpulses und Messung der Laufzeit des Echos
Entfernung (in m) ùëë =1/2  ùëê ùë° aus Laufzeit t (in s) des
√úbliche Frequenzen: 40kHz bis 200kHz

**Herausforderung 1: Identifikation des Impulses**

![RoboterSystem](images/UltraschallEchos.png "[^3]")

**Herausforderung 2: Querabh√§ngigkeiten**

Neben den Reflexionsmechanismen sind auch die Ausbreitungsparameter des Schallimpulses von der Umgebung abh√§ngig.
Die Schallgeschwindigkeit ist abh√§nig von

+ Temperatur
+ Luftdruck
+ Luftzusammensetzung (Anteil von CO$_2$, Luftfeuchte)

![RoboterSystem](images/Schallgeschwindigkeit.png "Schallgeschwindigkeit in Abh√§ngigkeit von der Temperatur und dem Luftdruck")

Welchen Einfluss haben diese Gr√∂√üen? F√ºr zwei Konfigurationen, die zwei unterschiedliche
Wetterlagen repr√§sentieren ergibt sich bereits ein Fehler von 8%.

$v_1 (980 hPa, 0¬∞) = 325\frac{m}{s}$

$v_2 (1040 hPa, 30¬∞) = 355\frac{m}{s}$

**Herausforderungen:**

+ Echoidentifikation (Mehrfachreflexionen)
+ **Temperaturabh√§ngigkeit**: $v_{Schall}(0¬∞C) = 331 m/s$, $v_{Schall}(30¬∞C) = 349 m/s$
+ √ñffnungswinkel (ca. 15-60¬∞)

### Phasenverschiebung

Die Phasenverschiebung des reflektierten Laserstrahls oder dessen Modulation gegen√ºber dem ausgesandten Strahl ist entfernungsabh√§ngig.

![RoboterSystem](images/Phasenverschiebung.png "Phasenverschiebung zwischen einem ausgesandten und dem empfangenen Signal- Wikimedia Commons, Autor Guy Muller https://de.wikipedia.org/wiki/Elektrooptische_Entfernungsmessung#/media/Datei:PhasenModulation.JPG")<!--style="width: 70%; max-width: 720px;"-->

Der zentrale Nachteil des Verfahrens besteht darin, dass die Messung des Phasenunterschieds oberhalb einer Phasendifferenz von mehr als 360 Grad wegen des periodischen Charakters keine eindeutige Aussage zum Abstand mehr zul√§sst.

$c=f\cdot \lambda$ f√ºr $f = 5 Mhz$ ergibt sich eine Wellenl√§nge von 60m.

Eine L√∂sung besteht darin verschiedene Frequenzen unterschiedlicher Wellenl√§nge durchzuschalten und durch logische Vergleiche der Messwerte eine gro√üe Reichweite und zudem eine hohe Genauigkeit erreichen.

### Triangulation

Triangulationsverfahren setzen auf einem bekannten Abstand zwischen von Empf√§nger und Sender auf. Die sogenannte *Baseline* ist dann Ausgangspunkt f√ºr die Bestimmung des Abstandes. An dieser Stelle seien zwei Beispiele gezeigt:

![RoboterSystem](images/Triangulation.png "Funktionsweise eines einfachen IR-Distanzsensors nach dem Triangulationsprinzip [Link](http://www.symmons.com/Press-Room/News/2010/november/S-6060-sensor-faucet.aspx)")<!-- width="60%" -->

Anwendung findet dieses Konzept auch bei RGB-D Kameras, sowohl bei Infrarotbasierten Systemen als auch bei Stereokameras.

### Positionsbestimmung: GNSS (GPS)

    --{{0}}--
Globale Navigationssatellitensysteme erm√∂glichen weltweite Positionsbestimmung. Prinzip: Laufzeitmessung zu mindestens 4 Satelliten. GNSS und GPS werden oft synonym verwendet, obwohl GPS nur eines von mehreren Systemen ist.

> Wie funktioniert eigentlich ein Global Navigation Satellite System (GNSS)? 

![GPS Konstellation](images/ConstellationGPS.gif "GPS-Satelliten  Wikipedia, Autor El pak")

**Systeme:**

+ GPS (USA)
+ Galileo (EU)
+ GLONASS (Russland)
+ BeiDou (China)

#### Mathematische Grundlagen der GNSS-Positionsbestimmung

Ein GNSS-Empf√§nger bestimmt seine Position (x, y, z) durch Messung der **Laufzeit** der Funksignale mehrerer Satelliten.

F√ºr jeden Satelliten (i) gilt:

$$
\rho_i = c \cdot (t_{\text{Empfang}} - t_{\text{Sendung}, i})
$$

Dabei ist

* ($\rho_i$): gemessene **Signal-Laufstrecke** (pseudo-range),
* ($c$): Lichtgeschwindigkeit,
* ($t_{\text{Empfang}}$): lokale Empfangszeit im Empf√§nger,
* ($t_{\text{Sendung}, i}$): Sendezeit des Satelliten.


Die Satelliten haben extrem genaue Atomuhren, dein Empf√§nger jedoch **nicht**.
Daher gibt es einen **Uhrfehler** ( $\Delta t_r$ ) des Empf√§ngers, der zu einer **zus√§tzlichen Unbekannten** f√ºhrt.

Damit lautet die Gleichung f√ºr Satellit (i):

$$
\rho_i = \sqrt{(x - x_i)^2 + (y - y_i)^2 + (z - z_i)^2} + c \cdot \Delta t_r
$$

mit den bekannten Satellitenpositionen ((x_i, y_i, z_i)).


Wir haben also vier **Unbekannte**:  (x, y, z) und ( $\Delta t_r$ ). Das resultierende nichtlineare Gleichungssystem:

$$
\begin{cases}
\rho_1 = \sqrt{(x - x_1)^2 + (y - y_1)^2 + (z - z_1)^2} + c \Delta t_r \\
\rho_2 = \sqrt{(x - x_2)^2 + (y - y_2)^2 + (z - z_2)^2} + c \Delta t_r \\
\rho_3 = \sqrt{(x - x_3)^2 + (y - y_3)^2 + (z - z_3)^2} + c \Delta t_r \\
\rho_4 = \sqrt{(x - x_4)^2 + (y - y_4)^2 + (z - z_4)^2} + c \Delta t_r
\end{cases}
$$

Diese vier Gleichungen k√∂nnen (z. B. iterativ durch lineare Approximation) gel√∂st werden.

#### Genauigkeit und Fehlerquellen bei GNSS

    --{{0}}--
Die Genauigkeit von GNSS-Systemen wird durch verschiedene Fehlerquellen beeinflusst.

**Fehlerquellen:**

+ Atmosph√§rische Effekte
+ Mehrwegeeffekte (Reflexionen)
+ Satellitenkonfiguration (DOP - Dilution of Precision)
+ Abschattung durch Geb√§ude

![DOP](images/Dilution_Of_Precision.svg.png "Geometrischer Einfluss - Dilution of Precision Wikipedia, Autor Xoneca")<!-- width="60%" -->

**Verbesserungen:**

+ **DGPS** (Differential GPS): Korrektursignale von Basisstationen
+ **SAPOS** (Deutschland): Genauigkeit bis < 1cm
+ **WAAS/EGNOS**: Satellitengest√ºtzte Korrekturen


## Zusammenfassung

    --{{0}}--
Fassen wir die wichtigsten Punkte zusammen:

**Was haben wir gelernt?**

+ **Sensorklassifikation**: Intern/extern, aktiv/passiv, Dimension
+ **Propriozeptive Sensoren**:
  - IMU (Beschleunigung, Gyro, Magnetometer)
  - Odometrie (Encoder)
+ **Exterozeptive Sensoren**:
  - Ultraschall (Laufzeit)
  - GPS (globale Position)
+ **Messprinzipien**: Amplitude, Laufzeit, Phase, Triangulation
+ **Fehlerquellen**: Drift, Temperatur, St√∂rungen, Geometrie

    --{{1}}--
In den kommenden Vorlesungen werden wir weitere Sensoren kennenlernen: Kameras (VL 6-7), Lidar und 3D-Sensoren (VL 8). Au√üerdem lernen wir, wie man Sensordaten fusioniert (VL 9-10), um robuste Sch√§tzungen zu erhalten.

      {{1}}
> **Vorbereitung f√ºr VL 3:** √úberlegen Sie sich, wie man verschiedene Sensoren r√§umlich zueinander in Beziehung setzen kann. Wenn ein Roboter eine Kamera vorne und einen Lidar hinten hat ‚Äì wie beschreibt man deren Position zueinander?

## Ausblick: VL 3 - Koordinatensysteme und Transformationen

    --{{0}}--
In der n√§chsten Vorlesung lernen Sie, wie man mit verschiedenen Koordinatensystemen umgeht und Sensordaten korrekt transformiert.

**Themen VL 3:**

+ Homogene Transformationen
+ Koordinaten-Frames
+ tf2 in ROS 2
+ URDF (Robot Description Format)
