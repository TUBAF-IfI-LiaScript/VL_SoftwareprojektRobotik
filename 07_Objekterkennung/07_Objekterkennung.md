<!--

author:   Sebastian Zug & Claude.ai
email:    sebastian.zug@informatik.tu-freiberg.de & Georg.Jaeger@informatik.tu-freiberg.de
version:  0.0.1
language: de
narrator: Deutsch Female

import: https://github.com/liascript/CodeRunner
        https://raw.githubusercontent.com/liascript-templates/plantUML/master/README.md
        https://raw.githubusercontent.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/refs/heads/master/config.md

-->

[![LiaScript](https://raw.githubusercontent.com/LiaScript/LiaScript/master/badges/course.svg)](https://liascript.github.io/course/?https://raw.githubusercontent.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/refs/heads/master/07_Objekterkennung/07_Objekterkennung.md)

# Objekterkennung und Tracking

| Parameter            | Kursinformationen                                                                                                     |
| -------------------- | --------------------------------------------------------------------------------------------------------------------- |
| **Veranstaltung:**   | @config.lecture                                                                                                       |
| **Semester**         | @config.semester                                                                                                      |
| **Hochschule:**      | `Technische UniversitÃ¤t Freiberg`                                                                                     |
| **Inhalte:**         | `Objekterkennung mit Features und Deep Learning, Tracking, 3D-Objekterkennung`                                        |
| **Link auf GitHub:** | https://github.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/blob/master/07_Objekterkennung/07_Objekterkennung.md |
| **Autoren**          | @author                                                                                                               |

![](https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif)

--------------------------------------------------------------------------------

**Zielstellung der heutigen Veranstaltung**

+ VerstÃ¤ndnis feature-basierter Objekterkennung (ORB, AKAZE)
+ Deep Learning fÃ¼r Object Detection (YOLOv8) - **Vorbereitung fÃ¼r Ãœbung 2**
+ Object Tracking Ã¼ber Bildsequenzen (DeepSORT)
+ 3D-Objekterkennung in Punktwolken (RANSAC, Clustering)
+ Integration in ROS 2 mit vision_msgs
+ **Praktische Anwendung: Personendetektion mit YOLOv8 + Stereo-PositionsschÃ¤tzung**

--------------------------------------------------------------------------------

## Motivation: Warum Objekterkennung?

    --{{0}}--
In den letzten beiden Vorlesungen haben wir gelernt, wie Kameras Bilder aufnehmen und wie wir aus Stereo-Bildern Tiefeninformationen gewinnen kÃ¶nnen. Heute geht es um die nÃ¤chste Stufe: Wie erkennen Roboter konkrete Objekte in ihrer Umgebung?

> Wir mÃ¶chten nicht nur sehen, sondern auch verstehen. Das bedeutet, dass wir Objekte identifizieren, lokalisieren und ihr Verhalten vorhersagen mÃ¼ssen, um darauf reagieren zu kÃ¶nnen.

```ascii
Kamerabild          Objekt        Verhalten
   â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”
   â”‚ â–‘â–‘â–“ â”‚  â”€â”€â”€â”€â”€> â”‚  ðŸš¶ â”‚ â”€â”€â”€â”€â”€> â”‚ Stop â”‚
   â”‚ â–“â–“â–‘ â”‚         â”‚ @3m â”‚        â”‚  !   â”‚
   â”‚ â–‘â–“â–“ â”‚         â””â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”˜                                                                                         .
```

    --{{0}}--
Ein Roboter muss nicht nur Pixel sehen, sondern verstehen: Was ist das? Wo ist es? Wie bewegt es sich? Diese FÃ¤higkeiten sind essentiell fÃ¼r autonome Navigation, Manipulation und Mensch-Roboter-Interaktion.

**Anwendungsbeispiele**

| Anwendung                     | Aufgabe                              | Beispiel                                   |
| ----------------------------- | ------------------------------------ | ------------------------------------------ |
| **Autonomes Fahren**          | FuÃŸgÃ¤nger, Fahrzeuge, Schilder       | Tesla Autopilot, Waymo                      |
| **Mobile Robotik**            | Hinderniserkennung, Person-Following | Serviceroboter in Hotels                    |
| **Industrielle Manipulation** | Objekterkennung fÃ¼r Pick-and-Place   | Bin-Picking in Lagerhallen                  |
| **Drohnen**                   | Landing Pad Detection                | Autonome Landung                            |
| **Soziale Robotik**           | Gesichtserkennung, Gestenerkennung   | Pepper, NAO                                 |

https://github.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/blob/master/00_Einfuehrung/images/ROSE2024_Chemnitz.pdf

> **Ãœbung 2 Kontext**: Wir werden YOLOv8 verwenden, um Personen in Kamerabildern zu erkennen und deren 3D-Position mit Stereo-Vision zu bestimmen!

### Begriffsdefinitionen

**Objektbezogene Perspective:**

| Begriff                              | Fragestellung               | Ergebnis                  | Beispiel                                       |
| ------------------------------------ | --------------------------- | ------------------------- | ---------------------------------------------- |
| **Detection** (Detektion)            | Wo ist ein Objekt?          | Bounding Box + Klasse     | "Person bei (320, 240), 80Ã—150 Pixel"          |
| **Classification** (Klassifikation)  | Was ist das Objekt?         | Klasse + Confidence       | "Person mit 95% Wahrscheinlichkeit"            |
| **Identification** (Identifikation)  | Wer/welches Individuum?     | ID innerhalb einer Klasse | "Das ist Max MÃ¼ller"                           |
| **Tracking** (Verfolgung)            | Wie bewegt sich das Objekt? | Trajektorie Ã¼ber Zeit     | "Person #42 bewegt sich mit 1 m/s nach rechts" |

```ascii
Klassifikation vs. Identifikation:

Klassifikation:                    Identifikation:
"Was ist es?" (Kategorie)          "Wer ist es?" (Individuum)

  ðŸš¶  ðŸš¶  ðŸš¶                         ðŸš¶  ðŸš¶  ðŸš¶
   â†“   â†“   â†“                          â†“   â†“   â†“
 Person Person Person               Max  Anna  Tom
 (alle gleiche Klasse)              (unterschiedliche IDs)                                                  .
```

**Pixelbasierte Perspektive (Segmentation):**

| Begriff                   | Fragestellung                           | Ergebnis       | Beispiel               |
| ------------------------- | --------------------------------------- | -------------- | ---------------------- |
| **Instance Segmentation** | Welche Pixel gehÃ¶ren zu welchem Objekt? | Maske pro Objekt | Mask R-CNN           |
| **Semantic Segmentation** | Welcher Klasse gehÃ¶rt jedes Pixel?      | Klassenmaske   | StraÃŸe, Gehweg, Himmel |



## Abgrenzung: Traditionelle vs. Deep Learning Methoden

In dieser Vorlesung behandeln wir **beide AnsÃ¤tze** der Objekterkennung:

| Aspekt | Traditionelle Methoden (Features) | Deep Learning (CNN, YOLO) |
|--------|-----------------------------------|---------------------------|
| **Ziel** | Geometrische Korrespondenzen | Semantische Klassifikation |
| **Anwendung** | SLAM, Visual Odometry, Stereo-Matching, Kamerakalibrierung | Objekterkennung, Autonomes Fahren |
| **Trainingsdaten** | Keine nÃ¶tig | GroÃŸe annotierte DatensÃ¤tze |
| **Rechenleistung** | CPU ausreichend | GPU erforderlich |
| **Interpretierbarkeit** | Transparent, mathematisch fundiert | "Black Box" |

**Warum beides lernen?**

1. **KomplementÃ¤re StÃ¤rken**: Feature-Methoden liefern prÃ¤zise geometrische Information (wo ist ein Punkt im 3D-Raum?), wÃ¤hrend Deep Learning semantisches VerstÃ¤ndnis bietet (was ist das Objekt?).

2. **Hybride Systeme**: Moderne Robotik-Anwendungen kombinieren oft beide AnsÃ¤tze - z.B. YOLO fÃ¼r Objekterkennung + ORB-Features fÃ¼r Tracking und Lokalisierung.

3. **Ressourcen-Constraints**: Auf eingebetteten Systemen sind klassische Methoden oft die einzige Option.

4. **GrundlagenverstÃ¤ndnis**: Die mathematischen Konzepte hinter Feature-Detection bilden die Basis fÃ¼r das VerstÃ¤ndnis moderner Architekturen.

## Feature-basierte Objekterkennung (2D)

    --{{0}}--
Traditionelle Objekterkennung basiert auf charakteristischen Merkmalen - sogenannten Features. Diese Methoden sind auch heute noch relevant, besonders fÃ¼r SLAM und visuelle Odometrie.

**Feature = charakteristischer, wiedererkennbarer Ausschnitt eines Bildes**

Eigenschaften guter Features:

+ **Repeatability**: Unter verschiedenen Bedingungen wiedererkennbar
+ **Distinctiveness**: Eindeutig unterscheidbar von anderen Features
+ **Locality**: Robust gegen Verdeckung
+ **Efficiency**: Schnell berechenbar
+ **Invariance**: UnabhÃ¤ngig von Rotation, Skalierung, Beleuchtung

| Gute Features | Schlechte Features    |
| ------------- | --------------------- |
| Ecken âœ“       | Glatte FlÃ¤chen âœ—      |
| Kanten âœ“      | RegelmÃ¤ÃŸige Muster âœ—  |
| Blobs âœ“       | Homogene Bereiche âœ—   |

> Warum wollen wir Ã¼berhaupt gleiche Features in verschiedenen Bildern finden? Weil wir so Korrespondenzen herstellen kÃ¶nnen, die fÃ¼r 3D-Rekonstruktion, BewegungsschÃ¤tzung und Objekterkennung essentiell sind!

### Corner Detection: Harris & Shi-Tomasi & FAST

    --{{0}}--
Ecken sind ideale Features, weil sie in zwei Richtungen starke Gradienten haben.

**Harris Corner Detector (1988)**

Idee: Suche Bereiche, wo das Bild in alle Richtungen stark variiert

**Schritt 1: Gradientenbilder berechnen**

FÃ¼r jeden Pixel wird der Gradient (HelligkeitsÃ¤nderung) durch Faltung des Graustufenbildes $I$ mit dem Sobel-Kernel berechnet:

$$
I_x = I * S_x \quad \text{mit} \quad S_x = \begin{bmatrix} -1 & 0 & +1 \\ -2 & 0 & +2 \\ -1 & 0 & +1 \end{bmatrix}
$$

$$
I_y = I * S_y \quad \text{mit} \quad S_y = \begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ +1 & +2 & +1 \end{bmatrix}
$$

Dabei ist $I$ das Eingabebild (Graustufenwerte 0-255), $*$ die Faltungsoperation und $S_x$, $S_y$ die Sobel-Kernel. Das Ergebnis sind zwei komplette Bilder $I_x$ und $I_y$ mit den Gradienten fÃ¼r jeden Pixel.

**Schritt 2: Struktur-Matrix M fÃ¼r jeden Pixel**

FÃ¼r jeden Pixel $(x_0, y_0)$ wird Ã¼ber ein lokales Fenster W summiert:

$$
M = \sum_{(x,y) \in W} \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix}
$$

```ascii
Bedeutung der Matrix-EintrÃ¤ge:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Î£ IxÂ²           â”‚ StÃ¤rke horizontaler Ã„nderungen     â”‚
â”‚ Î£ IyÂ²           â”‚ StÃ¤rke vertikaler Ã„nderungen       â”‚
â”‚ Î£ IxÂ·Iy         â”‚ Korrelation beider Richtungen      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Beispiele:

Vertikale Kante:        Horizontale Kante:      Ecke:
    â–‘â–‘â–ˆâ–ˆ                    â–‘â–‘â–‘â–‘                  â–‘â–‘â–ˆâ–ˆ
    â–‘â–‘â–ˆâ–ˆ                    â–ˆâ–ˆâ–ˆâ–ˆ                  â–ˆâ–ˆâ–ˆâ–ˆ

Ix: groÃŸ, Iy: klein     Ix: klein, Iy: groÃŸ    Ix: groÃŸ, Iy: groÃŸ
â†’ Nur eine Richtung     â†’ Nur eine Richtung    â†’ Beide Richtungen!
â†’ KANTE                 â†’ KANTE                â†’ ECKE âœ“                                                     .
```

> Wie kÃ¶nnen wir aber die "StÃ¤rke der Ecke" beschreiben? DafÃ¼r nutzen wir die Eigenwerte der Matrix M.

**Schritt 3: Corner Response Function**

Die Eigenwerte $\lambda_1$, $\lambda_2$ von M beschreiben die **Hauptrichtungen und StÃ¤rken der IntensitÃ¤tsÃ¤nderung**. Geometrisch definiert M eine Ellipse, deren Halbachsen durch die Eigenwerte gegeben sind:

```ascii
Flache Region:         Kante:                 Ecke:
(beide Î» klein)        (ein Î» groÃŸ)           (beide Î» groÃŸ)

      Â·                    |                    â”€â”¼â”€
    Â· Â· Â·                  |                    â”€â”¼â”€
      Â·                    |                    â”€â”¼â”€

Ellipse: winzig        Ellipse: lang/schmal   Ellipse: kreisfÃ¶rmig
Î»â‚ â‰ˆ Î»â‚‚ â‰ˆ 0            Î»â‚ >> Î»â‚‚               Î»â‚ â‰ˆ Î»â‚‚ >> 0                                                  .
```

| Situation | $\lambda_1$ | $\lambda_2$ | Bedeutung |
|-----------|-------------|-------------|-----------|
| **Flach** | klein | klein | Keine Ã„nderung in beide Richtungen |
| **Kante** | groÃŸ | klein | Starke Ã„nderung nur senkrecht zur Kante |
| **Ecke** | groÃŸ | groÃŸ | Starke Ã„nderung in **beide** Richtungen |

Aber ... Harris suchte eine LÃ¶sung ohne Eigenwertberechnung! Harris nutzt einen Trick - Determinante und Spur kodieren die Eigenwerte:

$$
R = \det(M) - k \cdot \text{trace}(M)^2
$$

Mit $k \approx 0.04-0.06$

**Warum keine Eigenwertberechnung?**

1. Numerische StabilitÃ¤t: Die Wurzelberechnung kann bei sehr kleinen oder sehr Ã¤hnlichen Eigenwerten zu numerischen Problemen fÃ¼hren (Division durch kleine Zahlen, Rundungsfehler)
2. Ausreichend fÃ¼r die Aufgabe: Harris braucht keine exakten Eigenwerte - er braucht nur eine Entscheidungsfunktion die sagt: "Ecke oder nicht". Die Kombination $\det(M) - k \cdot \text{trace}(M)^2$ liefert genau das, ohne die Eigenwerte explizit zu kennen.
3. Historisch (1988): Damals war die Rechenperformance tatsÃ¤chlich noch ein relevanter Faktor.

> Der Harris-Detektor ist rotationsinvariant, weil Determinante und Spur bei Rotation der Matrix erhalten bleiben:

**Shi-Tomasi (1994) - "Good Features to Track"**

Verbesserte Version: Verwendet direkt die kleinere Eigenwerte

$$
R = \min(\lambda_1, \lambda_2)
$$

Wenn $R > \text{threshold}$: Guter Feature-Punkt

**Visualisierung:**

> Diese Corner-Detektoren werden in SLAM-Systemen fÃ¼r Feature-Tracking verwendet!


```python -loadImage.py 
import cv2
import numpy as np

import urllib.request

def load_image_from_url(url):
    resp = urllib.request.urlopen(url)
    image_data = resp.read()
    image_array = np.asarray(bytearray(image_data), dtype=np.uint8)
    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
    return image
```
```python corner_detection.py
import cv2
import numpy as np
from loadImage import load_image_from_url

image = load_image_from_url('https://r4r.informatik.tu-freiberg.de/content/images/size/w960/2022/08/karl_kegel_bau1.jpg')

# Graustufenkonvertierung (notwendig fÃ¼r Corner Detection)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Harris Corner Detector
harris_response = cv2.cornerHarris(np.float32(gray), blockSize=2, ksize=3, k=0.04)

# Graustufenbild in BGR konvertieren fÃ¼r farbige Markierungen
image_harris = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
image_harris[harris_response > 0.01 * harris_response.max()] = [0, 0, 255]

# Shi-Tomasi Corner Detector ("Good Features to Track")
corners = cv2.goodFeaturesToTrack(gray, maxCorners=100, qualityLevel=0.01, minDistance=10)

# Graustufenbild in BGR konvertieren fÃ¼r farbige Markierungen
image_shi_tomasi = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
if corners is not None:
    for corner in corners:
        x, y = corner.ravel()
        cv2.circle(image_shi_tomasi, (int(x), int(y)), 5, (0, 255, 0), -1)

cv2.imwrite('harris_corners.png', image_harris)
cv2.imwrite('shi_tomasi_corners.png', image_shi_tomasi)
```
@LIA.eval(`["loadImage.py", "main.py"]`, `none`, `python3 main.py`, `*`)

> Immer noch zu viel Rechenaufwand fÃ¼r den Einsatz auf mobilen Robotern? Dann schauen wir uns jetzt einen extrem schnellen Detektor an: FAST!

**FAST (Features from Accelerated Segment Test)**

1. WÃ¤hle einen Pixel $p$ mit IntensitÃ¤t $I_p$
2. Betrachte 16 Pixel auf einem Kreis (Radius 3) um $p$
3. Ist $p$ eine Ecke, wenn $n$ aufeinanderfolgende Pixel heller/dunkler sind
4. Typisch: $n = 12$, Threshold $t = 20$

![](https://docs.opencv.org/3.4/fast_speedtest.jpg "OpenCV FAST Example")

Jedem der 16 Pixel wird ein Label zugewiesen:

+ Heller ($I_{kreis} > I_p + t$)
+ Dunkler ($I_{kreis}< I_p - t$)
+ Ã„hnlich (weder noch)

Dann wird geprÃ¼ft: Gibt es 12 zusammenhÃ¤ngende Pixel, die alle dasselbe Label (heller oder dunkler) haben? Falls ja â†’ Ecke erkannt.

### Feature Descriptoren: Rotated BRIEF

> Nachdem wir nun Ecken detektieren kÃ¶nnen, brauchen wir eine MÃ¶glichkeit, diese Ecken zu beschreiben, damit wir sie in verschiedenen Bildern wiedererkennen kÃ¶nnen. HierfÃ¼r verwenden wir Deskriptoren.

**BRIEF (Binary Robust Independent Elementary Features)**

Beschreibt eine Ecke durch BinÃ¤rvergleiche:

1. WÃ¤hle $n$ Pixelpaare zufÃ¤llig um die Ecke
2. Vergleiche deren IntensitÃ¤ten
3. Speichere Ergebnis als Bit-String

$$
\text{BRIEF}(p) = \sum_{1 \leq i \leq n} 2^{i-1} \cdot \tau(p; x_i, y_i)
$$

Wobei:
$$
\tau(p; x, y) = \begin{cases} 1 & \text{wenn } I(p_x) < I(p_y) \\ 0 & \text{sonst} \end{cases}
$$

Typisch: $n = 256$ â†’ 256-Bit-Deskriptor

https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf

> **Kernidee**: BRIEF vergleicht IntensitÃ¤ten von Pixelpaaren und speichert das Ergebnis als Bit. Zwei Deskriptoren werden durch die Hamming-Distanz (Anzahl unterschiedlicher Bits) verglichen - sehr schnell durch XOR-Operation!

**Limitierung von BRIEF:** Nicht rotationsinvariant!

Das folgende Beispiel zeigt, wie BRIEF funktioniert: FÃ¼r einen Feature-Punkt werden zufÃ¤llige Pixelpaare verglichen und das Ergebnis als Bit-String gespeichert.

```python -loadImage.py
import cv2
import numpy as np
import urllib.request

def load_image_from_url(url):
    resp = urllib.request.urlopen(url)
    image_data = resp.read()
    image_array = np.asarray(bytearray(image_data), dtype=np.uint8)
    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
    return image
```
```python brief_demo.py
import cv2
import numpy as np
from loadImage import load_image_from_url

# Bild laden und in Graustufen konvertieren
image = load_image_from_url('https://r4r.informatik.tu-freiberg.de/content/images/size/w960/2022/08/karl_kegel_bau1.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (5, 5), 0)  # GlÃ¤ttung fÃ¼r StabilitÃ¤t

# Feature-Punkt (Zentrum fÃ¼r Demo)
feature_point = (gray.shape[1] // 2, gray.shape[0] // 2)
patch_size = 31  # Typische Patch-GrÃ¶ÃŸe fÃ¼r BRIEF

# Generiere zufÃ¤llige Pixelpaare (vereinfachte BRIEF-Variante)
np.random.seed(42)  # Reproduzierbarkeit
n_pairs = 16  # Reduziert fÃ¼r Visualisierung (normal: 256)
pairs = np.random.randint(-patch_size//2, patch_size//2, size=(n_pairs, 4))

# Berechne BRIEF-Deskriptor
def compute_brief(img, point, pairs):
    """Berechne einen vereinfachten BRIEF-Deskriptor"""
    descriptor = []
    px, py = point

    for i, (dx1, dy1, dx2, dy2) in enumerate(pairs):
        # Koordinaten der beiden Pixel im Paar
        x1, y1 = px + dx1, py + dy1
        x2, y2 = px + dx2, py + dy2

        # Bounds-Check
        if (0 <= x1 < img.shape[1] and 0 <= y1 < img.shape[0] and
            0 <= x2 < img.shape[1] and 0 <= y2 < img.shape[0]):
            # Vergleich: I(p1) < I(p2) => 1, sonst 0
            bit = 1 if img[y1, x1] < img[y2, x2] else 0
            descriptor.append(bit)
        else:
            descriptor.append(0)

    return descriptor

# Berechne Deskriptor
descriptor = compute_brief(gray, feature_point, pairs)

# Visualisierung
vis_image = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
px, py = feature_point

# Zeichne Feature-Punkt
cv2.circle(vis_image, (px, py), 8, (0, 255, 0), 2)

# Zeichne einige Pixelpaare
colors = [(255, 0, 0), (0, 0, 255)]  # Blau = 0, Rot = 1
for i, (dx1, dy1, dx2, dy2) in enumerate(pairs[:8]):  # Nur erste 8 fÃ¼r Ãœbersichtlichkeit
    x1, y1 = px + dx1, py + dy1
    x2, y2 = px + dx2, py + dy2
    color = colors[descriptor[i]]
    cv2.line(vis_image, (x1, y1), (x2, y2), color, 1)
    cv2.circle(vis_image, (x1, y1), 3, color, -1)
    cv2.circle(vis_image, (x2, y2), 3, color, -1)

# Ergebnis ausgeben
print("BRIEF-Deskriptor Demonstration")
print("=" * 40)
print(f"Feature-Punkt: ({px}, {py})")
print(f"Anzahl Bit-Vergleiche: {n_pairs}")
print(f"\nBinÃ¤rer Deskriptor: {''.join(map(str, descriptor))}")
print(f"Als Integer: {int(''.join(map(str, descriptor)), 2)}")
print(f"\nVergleich zweier Deskriptoren (Hamming-Distanz):")

# Demonstriere Hamming-Distanz
descriptor2 = compute_brief(gray, (px + 5, py + 5), pairs)  # Leicht verschoben
hamming = sum(a != b for a, b in zip(descriptor, descriptor2))
print(f"Deskriptor 1: {''.join(map(str, descriptor))}")
print(f"Deskriptor 2: {''.join(map(str, descriptor2))}")
print(f"Hamming-Distanz: {hamming} Bits unterschiedlich")

cv2.imwrite('brief_visualization.png', vis_image)
print("\nVisualisierung gespeichert: brief_visualization.png")
print("(Blaue Linien = Bit 0, Rote Linien = Bit 1)")
```
@LIA.eval(`["loadImage.py", "main.py"]`, `none`, `python3 main.py`, `*`)

**Gibt es auch eine rotationsinvariante Umsetzung?**

ORB (Oriented FAST and Rotated BRIEF) ist die rotationsinvariante Variante von BRIEF:


1. Orientierung berechnen: FÃ¼r jeden Keypoint wird die dominante Orientierung mittels Intensity Centroid bestimmt:
$$\theta = \arctan2(m_{01}, m_{10})$$ wobei $m_{01}$ und $m_{10}$ die Bildmomente im Patch sind.

> **Was sind Bildmomente?** Die Bildmomente beschreiben den "Schwerpunkt der IntensitÃ¤t" in einem kreisfÃ¶rmigen Patch (typisch Radius $r = 15$) um den Keypoint:
>
> ```ascii
> FAST-Detektion:              Moment-Berechnung:
>
>      Â· Â· Â· Â· Â·                  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
>     Â·         Â·                â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
>    Â·           Â·              â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
>    Â·     â—     Â·      â†’       â–‘â–‘â–‘â–‘â–‘â–‘â–‘â—â–‘â–‘â–‘â–‘â–‘â–‘â–‘
>    Â·           Â·              â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
>     Â·         Â·                â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
>      Â· Â· Â· Â· Â·                  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
>
>    16 Pixel (r=3)             Voller Patch (r=15)
>    fÃ¼r Corner-Test            fÃ¼r Orientierung                                                            .
> ```
>
> **Analogie zur Mechanik:** Der Intensity Centroid entspricht dem Schwerpunkt eines KÃ¶rpers, wobei PixelintensitÃ¤ten die Rolle der Massen Ã¼bernehmen:
>
> | Mechanik | Bildverarbeitung |
> |----------|------------------|
> | Masse $m_i$ an Position $(x_i, y_i)$ | IntensitÃ¤t $I(x,y)$ an Pixel $(x,y)$ |
> | Gesamtmasse $M = \sum m_i$ | $m_{00} = \sum I(x,y)$ |
> | Schwerpunkt $\bar{x} = \frac{\sum m_i x_i}{M}$ | Centroid $C_x = \frac{m_{10}}{m_{00}}$ |

2. Rotierte Pixelpaare: Die vordefinierten BRIEF-Pixelpaare werden entsprechend der Orientierung $\theta$ rotiert, bevor der Deskriptor berechnet wird.
3. Berechnung des Deskriptors: Wie bei BRIEF, aber mit den rotierten Pixelpaaren.

**Weitere Deskriptoren (Float-basiert):**

Neben binÃ¤ren Deskriptoren wie ORB/BRIEF gibt es Float-Deskriptoren, die kontinuierliche Werte speichern:

<!-- data-type="none" -->
| Deskriptor | Dimension | Prinzip | Besonderheit |
|------------|-----------|---------|--------------|
| **SIFT** | 128 floats | Histogramme der Gradientenrichtungen in 4Ã—4 Subregionen | Skalen- und rotationsinvariant, patentiert bis 2020 |
| **SURF** | 64 floats | Haar-Wavelet-Antworten in Subregionen | Schneller als SIFT, Ã¤hnliche QualitÃ¤t |
| **AKAZE** | variabel | Nichtlineare SkalierungsrÃ¤ume mit Modified Local Difference Binary | Open-source Alternative zu SIFT |

> **Wann welchen Deskriptor?**
>
> - **ORB/BRIEF**: Echtzeitanwendungen (SLAM, Tracking) - schnell durch Hamming-Distanz
> - **SIFT/SURF**: Wenn Genauigkeit wichtiger als Geschwindigkeit ist (Panorama-Stitching, 3D-Rekonstruktion)
> - **AKAZE**: Guter Kompromiss - robust und frei verfÃ¼gbar

### Feature Matching

    --{{0}}--
Nachdem wir Features in zwei Bildern gefunden haben, mÃ¼ssen wir korrespondierende Punkte finden.

**Brute-Force Matcher**

Einfachster Ansatz: Vergleiche jeden Deskriptor mit jedem anderen

+ FÃ¼r binÃ¤re Deskriptoren (ORB, BRIEF): **Hamming-Distanz**
+ FÃ¼r Float-Deskriptoren (SIFT, SURF): **Euklidische Distanz**

Hamming-Distanz = Anzahl unterschiedlicher Bits

```python
# Brute-Force Matcher fÃ¼r ORB (Hamming)
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(descriptors1, descriptors2)

# Sortiere nach Distanz
matches = sorted(matches, key=lambda x: x.distance)
```

**Interaktives Beispiel: Ã„hnlichste Ecken mit ORB finden**

Das folgende Beispiel findet alle ORB-Features in einem Bild und sucht die Ã¤hnlichsten Ecken basierend auf der Hamming-Distanz ihrer Deskriptoren. Die Implementierung:

1. **Detektion**: ORB extrahiert bis zu 100 Keypoints mit FAST-Detektor
2. **Deskription**: FÃ¼r jeden Keypoint wird ein 256-Bit BRIEF-Deskriptor berechnet (32 Bytes)
3. **Matching**: Alle Paare werden verglichen - die Hamming-Distanz zÃ¤hlt unterschiedliche Bits
4. **Visualisierung**: Die 10 Ã¤hnlichsten Paare (kleinste Distanz) werden grÃ¼n markiert

> **Beobachtung:** Ã„hnliche Deskriptoren bedeuten Ã¤hnliche lokale Textur - nicht zwingend semantisch gleiche Objekte. Fensterecken links und rechts haben oft Ã¤hnliche Gradienten und werden daher als "Ã¤hnlich" erkannt.

```python -loadImage.py
import cv2
import numpy as np
import urllib.request

def load_image_from_url(url):
    resp = urllib.request.urlopen(url)
    image_data = resp.read()
    image_array = np.asarray(bytearray(image_data), dtype=np.uint8)
    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
    return image
```
```python orb_similar_corners.py
import cv2
import numpy as np
from loadImage import load_image_from_url

# Bild laden
image = load_image_from_url('https://r4r.informatik.tu-freiberg.de/content/images/size/w960/2022/08/karl_kegel_bau1.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# ORB-Detektor erstellen
orb = cv2.ORB_create(nfeatures=100)

# Keypoints und Deskriptoren berechnen
keypoints, descriptors = orb.detectAndCompute(gray, None)
print(f"Gefundene Keypoints: {len(keypoints)}")
print(f"Deskriptor-Shape: {descriptors.shape} (100 Keypoints x 32 Bytes = 256 Bits)")

# Alle Paare mit Hamming-Distanz berechnen
pairs = []
for i in range(len(descriptors)):
    for j in range(i + 1, len(descriptors)):
        distance = cv2.norm(descriptors[i], descriptors[j], cv2.NORM_HAMMING)
        pairs.append((distance, i, j))

# Nach Distanz sortieren und die 10 besten auswÃ¤hlen
pairs.sort(key=lambda x: x[0])
best_pairs = pairs[:10]

# Indizes der Keypoints in den besten Paaren sammeln
best_indices = set()
for dist, i, j in best_pairs:
    best_indices.add(i)
    best_indices.add(j)

print(f"\nDie 10 Ã¤hnlichsten Paare:")
for rank, (dist, i, j) in enumerate(best_pairs, 1):
    kp_i, kp_j = keypoints[i], keypoints[j]
    similarity = 100 * (1 - dist/256)
    print(f"  {rank}. KP {i} ({kp_i.pt[0]:.0f},{kp_i.pt[1]:.0f}) <-> "
          f"KP {j} ({kp_j.pt[0]:.0f},{kp_j.pt[1]:.0f}): "
          f"Distanz={int(dist)}, Ã„hnlichkeit={similarity:.1f}%")

# Visualisierung
vis_image = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)

# Alle Keypoints in Grau zeichnen
for i, kp in enumerate(keypoints):
    if i not in best_indices:
        cv2.circle(vis_image, (int(kp.pt[0]), int(kp.pt[1])), 4, (128, 128, 128), 1)

# Die Keypoints der 10 besten Paare in GrÃ¼n hervorheben
for idx in best_indices:
    kp = keypoints[idx]
    cv2.circle(vis_image, (int(kp.pt[0]), int(kp.pt[1])), 8, (0, 255, 0), 2)

cv2.imwrite('orb_similar_corners.png', vis_image)
print("\nVisualisierung gespeichert: orb_similar_corners.png")
print("(GrÃ¼ne Kreise = Keypoints der 10 Ã¤hnlichsten Paare)")
```
@LIA.eval(`["loadImage.py", "main.py"]`, `none`, `python3 main.py`, `*`)

> **Beobachtung:** Die Ã¤hnlichsten Ecken haben oft eine Ã¤hnliche lokale Struktur - z.B. zwei Fensterecken oder zwei GebÃ¤udekanten mit Ã¤hnlicher Textur.

**RANSAC fÃ¼r robuste Geometrie**

RANSAC folgt auf das Deskriptor-Matching und wirkt als geometrischer Konsistenzfilter auf die Matches.

Entferne Outliers durch geometrische Konsistenz auf der Basis von Essenzial- oder Homographiemodellen:

1. WÃ¤hle zufÃ¤llig minimale Menge von Matches
2. Berechne Homographie $H$ (oder Fundamentalmatrix $F$)
3. ZÃ¤hle Inliers (Matches konsistent mit $H$)
4. Wiederhole und wÃ¤hle beste LÃ¶sung

```python
# RANSAC fÃ¼r Homographie
H, mask = cv2.findHomography(
    src_pts, dst_pts,
    cv2.RANSAC,
    ransacReprojThreshold=5.0
)

# mask[i] == 1: Inlier, mask[i] == 0: Outlier
inliers = src_pts[mask.ravel() == 1]
```

> Wir varieren die Modellparameter und suchen nach der besten LÃ¶sung mit den meisten Inliers! RANSAC arbeitet nicht auf den Des

!?[](https://www.youtube.com/watch?v=EwlKwbyK8GI)

### Haar Features und Gesichtserkennung

    --{{0}}--
Ein weiterer klassischer Algorithmus fÃ¼r Objekterkennung sind Haar-artige Features, die durch Viola und Jones 2001 bekannt wurden. Diese Methode war bahnbrechend fÃ¼r die Echtzeit-Gesichtserkennung und wird in OpenCV als Haar Cascade Classifier implementiert.

**Grundidee der Haar Features:**

Haar Features basieren auf der Berechnung von Helligkeitsunterschieden zwischen benachbarten Rechteckbereichen im Bild:

```ascii
Haar Feature Typen:

  Edge Features:          Line Features:         Four-Rectangle:
  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚          â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚
  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚          â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜    â”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
                                                â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
  Vertikal   Horizontal

  Feature-Wert = Î£(weiÃŸe Pixel) - Î£(schwarze Pixel)                                                      .
```

Diese einfachen Features kÃ¶nnen komplexe Strukturen erfassen:

+ **Kantenfeatures**: Erkennen ÃœbergÃ¤nge zwischen hellen und dunklen Bereichen
+ **Linienfeatures**: Erkennen dunkle Linien auf hellem Hintergrund (z.B. Augenbrauen)
+ **Rechteckfeatures**: Erkennen kontrastierende Bereiche (z.B. Nase heller als Augenpartie)

Parameter pro Feature in einer Stage sind z.B.:

+ Position (x, y) im Bildfenster
+ GrÃ¶ÃŸe (Breite, HÃ¶he) der Rechtecke
+ Art des Features (Edge horizontal, Edge vertikal, Line etc.)
+ Gewicht (wie wichtig dieses Feature ist)
+ Schwellenwerte (Thresholds) zur Entscheidungsfindung

**Integralbilder fÃ¼r schnelle Berechnung:**

Der Trick fÃ¼r Echtzeit-Performance liegt in der Verwendung von Integralbildern:

$$II(x,y) = \sum_{x' \leq x, y' \leq y} I(x', y')$$

Mit dem Integralbild kann die Summe jedes Rechtecks in **konstanter Zeit O(1)** berechnet werden:

```ascii
Rechteck-Summe mit 4 Array-Zugriffen:

     A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ B
     â”‚           â”‚
     â”‚  Rechteck â”‚     Summe = II(D) - II(B) - II(C) + II(A)
     â”‚           â”‚
     C â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ D                                                                                     .
```

**AdaBoost Cascade Classifier:**

Das Training verwendet AdaBoost, um aus tausenden mÃ¶glicher Haar Features die relevantesten auszuwÃ¤hlen und in einer Kaskade anzuordnen:

```ascii
Cascade Structure:

  Bild-       Stage 1        Stage 2        Stage 3        Face
  Region  â”€â”€â”€ (wenige    â”€â”€â”€ (mehr     â”€â”€â”€ (viele    â”€â”€â”€ Detected!
               Features)      Features)      Features)

     â”‚            â”‚              â”‚              â”‚
     â–¼            â–¼              â–¼              â–¼
  Reject      Reject         Reject         Reject
  (schnell)   (schnell)      (langsam)      (langsam)                                                   .
```

+ FrÃ¼he Stufen haben wenige Features â†’ schnelle Ablehnung von Nicht-Gesichtern
+ SpÃ¤tere Stufen werden nur fÃ¼r vielversprechende Kandidaten ausgefÃ¼hrt
+ ~95% der Bildregionen werden bereits in den ersten Stufen verworfen

**OpenCV Implementierung:**

```python
import cv2

# Lade vortrainierten Haar Cascade Classifier
face_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
)

# Bild laden und in Graustufen konvertieren
img = cv2.imread('gruppe.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Gesichtserkennung durchfÃ¼hren
faces = face_cascade.detectMultiScale(
    gray,
    scaleFactor=1.1,    # Skalierungsfaktor zwischen Scans
    minNeighbors=5,     # Mindestanzahl benachbarter Detektionen
    minSize=(30, 30)    # Minimale GesichtsgrÃ¶ÃŸe
)

# Ergebnisse visualisieren
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)

print(f"Gefundene Gesichter: {len(faces)}")
cv2.imwrite('detected_faces.jpg', img)
```

**Parameter erklÃ¤rt:**

| Parameter | Bedeutung | Typischer Wert |
|-----------|-----------|----------------|
| `scaleFactor` | Verkleinerungsfaktor pro Scan-Durchlauf | 1.05 - 1.3 |
| `minNeighbors` | Erforderliche Ãœberlappungen fÃ¼r Detektion | 3 - 6 |
| `minSize` | Minimale ObjektgrÃ¶ÃŸe in Pixeln | (30, 30) |
| `maxSize` | Maximale ObjektgrÃ¶ÃŸe in Pixeln | unbegrenzt |

**Weitere verfÃ¼gbare Cascade Classifier in OpenCV:**

+ `haarcascade_eye.xml` - Augenerkennung
+ `haarcascade_smile.xml` - LÃ¤cheln-Erkennung
+ `haarcascade_profileface.xml` - Gesichter im Profil
+ `haarcascade_fullbody.xml` - GanzkÃ¶rper-Erkennung
+ `haarcascade_upperbody.xml` - OberkÃ¶rper

!?[](https://www.youtube.com/watch?v=hPCTwxF0qf4&t=103s)

## Deep Learning fÃ¼r Objekterkennung

> Deep Learning ist eine Kategorie des maschinellen Lernens, der auf tiefen kÃ¼nstlichen neuronalen Netzen basiert. â€žDeepâ€œ bezeichnet die Anzahl hintereinandergeschalteter Verarbeitungsebenen (Layer) in einem neuronalen Netz.

+ Viele hintereinandergeschaltete Schichten
+ Automatische Merkmalsextraktion
+ Hoher Daten- und Rechenbedarf
+ Besonders erfolgreich bei unstrukturierten Daten

```ascii
KÃ¼nstliche Intelligenz
 â””â”€ Maschinelles Lernen
     â””â”€ Deep Learning                                                   .
```

> â€žâ€šDeepâ€˜ bezeichnet die Tiefe der ReprÃ¤sentationshierarchie: Viele aufeinanderfolgende nichtlineare Transformationen, die aus Rohdaten schrittweise abstrakte Merkmale formen.â€œ

| Aspekt          | Haar Cascade                      | Deep Learning (YOLO)    |
| --------------- | --------------------------------- | ----------------------- |
| Geschwindigkeit | Sehr schnell (CPU)                | Schnell (benÃ¶tigt GPU)  |
| Genauigkeit     | Gut fÃ¼r frontale Gesichter        | Besser bei Variationen  |
| Robustheit      | Empfindlich auf Rotation          | Robust                  |
| Training        | Aufwendig, benÃ¶tigt viele Samples | End-to-End Training     |
| Speicherbedarf  | Sehr gering (`~1 MB`)             | GrÃ¶ÃŸer (`~10-100 MB`)     |
| Anwendungsfall  | Embedded Systems, Echtzeit        | Server, komplexe Szenen |

> **Fazit:** Haar Cascade Classifier sind ein gutes Beispiel dafÃ¼r, wie mit cleveren mathematischen Tricks (Integralbilder) und Machine Learning (AdaBoost) effiziente Objekterkennung mÃ¶glich ist. FÃ¼r viele Embedded-Anwendungen sind sie aufgrund ihrer Geschwindigkeit und geringen Ressourcenanforderungen immer noch relevant.

### DL Architekturen

> CNNs sind nicht das einzige Werkzeug des Deep Learning!

| Typ                                     | Typische Anwendungen                 | Beispiele fÃ¼r Anwendungen                                                                                                             |
| --------------------------------------- | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |
| **CNN (Convolutional Neural Networks)** | Bilder, Videos, rÃ¤umliche Daten      | Objekterkennung (z.B. YOLO, Gesichtserkennung), Bildklassifikation (z.B. Katzen vs. Hunde), medizinische Bildanalyse (Tumorerkennung) |
| **RNN (Recurrent Neural Networks)**     | Zeitreihen, Sprache, Sequenzen       | Handschriftenerkennung, maschinelle Ãœbersetzung, Sprachgenerierung, Aktienkursvorhersage                                              |
| **Transformers**                        | Sprache (NLP), Bild, multimodal      | Chatbots (z.B. ChatGPT), maschinelle Ãœbersetzung, Textzusammenfassung, Bildunterschriften-Generierung                                 |
| **Fully Connected / Dense Nets**        | Klassifikation, einfache ML-Aufgaben | KreditwÃ¼rdigkeitsprÃ¼fung, Spam-Erkennung, Iris-Blumenklassifikation (ein klassisches ML-Beispiel)                                     |
| **Autoencoder, GANs, etc.**             | Datenkompression, Generierung        | Bildrauschen entfernen, Bilderzeugung (DeepFakes, Stiltransfer), Anomalieerkennung in Produktionsdaten                                |

### CNN-Grundlagen (Kurzer Einstieg)

**Convolutional Neural Network (CNN):**

```ascii
Input Image          Conv Layer        Pooling         FC Layer      Output
  â”Œâ”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”
  â”‚     â”‚  â”€â”€â”€â”€â”€â”€>  â”‚     â”‚  â”€â”€â”€â”€â”€â”€> â”‚   â”‚ â”€â”€â”€â”€â”€â”€>  â”‚     â”‚ â”€â”€â”€>  â”‚Cat  â”‚
  â”‚ ðŸ±  â”‚  Filter   â”‚ â–“â–“â–“ â”‚  MaxPool â”‚ â–“ â”‚ Flatten  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚       â”‚Dog  â”‚
  â”‚     â”‚           â”‚ â–“â–“â–“ â”‚          â”‚ â–“ â”‚          â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚       â”‚Bird â”‚
  â””â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜
  224Ã—224Ã—3         112Ã—112Ã—64       56Ã—56Ã—64       4096          Classes                                  .
```

**Input Image (Eingabebild):**
Ein Farbbild mit z.B. **224Ã—224 Pixeln** und **3 FarbkanÃ¤len** (Rot, GrÃ¼n, Blau).
Das Bild wird als 3D-Array betrachtet: HÃ¶he Ã— Breite Ã— FarbkanÃ¤le.

**Convolutional Layer (Faltungsschicht):**
Viele kleine Filter (z.B. 3Ã—3) werden Ã¼ber das Bild geschoben.

* Jeder Filter erkennt lokale Merkmale wie Kanten, Ecken, Farben oder Texturen.
* Filterwerte werden **wÃ¤hrend des Trainings gelernt** und sind damit flexibel.
* Das Ergebnis sind mehrere **Feature Maps** (hier z.B. 64), die anzeigen, wo im Bild bestimmte Muster auftreten.

*Analogie:* Eine Schablone, die verschiedene Muster Ã¼ber das Bild legt und prÃ¼ft, wo sie am besten passen.

**Pooling Layer (Pooling-Schicht):**
Reduziert die rÃ¤umlichen Dimensionen der Feature Maps (hier von 112Ã—112 auf 56Ã—56).

* Typisch ist **Max-Pooling**, bei dem in kleinen Bereichen (z.B. 2Ã—2) der grÃ¶ÃŸte Wert Ã¼bernommen wird.
* Dadurch bleibt die wichtigste Information erhalten, wÃ¤hrend die Darstellung kompakter wird.
* Pooling macht das Modell robuster gegenÃ¼ber kleinen Verschiebungen und reduziert Rechenaufwand.

*Analogie:* Eine Zusammenfassung oder VergrÃ¶berung, die das Bild kompakter und Ã¼bersichtlicher macht.

**ReLU Activation (Aktivierungsfunktion) zwischen den Schichten:**
FÃ¼hrt eine einfache nichtlineare Transformation durch: $f(x) = \max(0, x)$

Diese NichtlinearitÃ¤t ermÃ¶glicht es dem Netzwerk, komplexe Muster zu lernen.

*Hinweis:* ReLU ist meist **kein eigener Layer**, sondern wird direkt nach jeder Conv- oder FC-Schicht angewendet.

**Fully Connected Layer (VollstÃ¤ndig verbundene Schicht):**
Am Ende des Netzes werden alle extrahierten Merkmale **flachgedrÃ¼ckt (flattened)** und als Vektor in den FC-Layer eingespeist.

* Jeder Eingang ist mit jedem Neuron verbunden.
* Der FC-Layer kombiniert alle Merkmale und entscheidet, zu welcher Klasse (z.B. Katze, Hund, Vogel) das Bild gehÃ¶rt.

> Was bedeutet â€žDeepâ€œ genau in deinem CNN-Diagramm?

â€žDeepâ€œ bedeutet, dass viele Convolutional Layers (plus weitere Schichten wie Pooling und Aktivierung) hintereinander geschaltet werden, um eine Hierarchie von Merkmalen zu lernen â€” von einfachen Kanten bis zu komplexen Objekten.
Diese Tiefe ermÃ¶glicht es dem Netzwerk, sehr komplexe Muster und ZusammenhÃ¤nge in den Daten zu erfassen.

### YOLO: You Only Look Once

YOLO ist ein **Single-Shot Detector**: Das gesamte Bild wird in einem einzigen Durchgang verarbeitet, und alle Objekte werden gleichzeitig erkannt. Das macht YOLO extrem schnell!

**YOLO-Prinzip:**

1. **Teile das Bild in ein Grid** (z.B. 13Ã—13 Zellen)
2. **Jede Grid-Zelle sagt vorher:**

   * $B$ Bounding Boxes mit Koordinaten ((x, y, w, h)) und Confidence-Wert
   * $C$ Klassenwahrscheinlichkeiten
3. **Non-Maximum Suppression (NMS)** entfernt mehrfach erkannte Objekte (Ãœberlappungen)

```ascii
Grid-basierte Objekterkennung:

  â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤        Grid-Zelle (3,2) detektiert:
  â”œâ”€â”¼â”€â”¼â–ˆâ”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤        â€¢ Bounding Box: (x=3.2, y=2.7, w=2.1, h=3.5)
  â”œâ”€â”¼â”€â”¼â–ˆâ”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â”€â”€â”€>  â€¢ Confidence: 0.89
  â”œâ”€â”¼â”€â”¼â–ˆâ”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤        â€¢ Klasse: "person"
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
  â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜
   7Ã—7 Grid                                                                                               .
```

**Evolution der YOLO-Familie (Auswahl):**

| Version | Jahr | mAP (COCO) | FPS | Besonderheiten                  |
| ------- | ---- | ---------- | --- | ------------------------------- |
| YOLOv1  | 2016 | ~63.4%     | 45  | Erste Single-Shot Detection     |
| YOLOv2  | 2017 | ~78.6%     | 67  | Batch Norm, Anchor Boxes        |
| YOLOv3  | 2018 | ~57.9%     | 45  | Multi-Scale Predictions         |
| YOLOv4  | 2020 | ~65.7%     | 65  | CSPDarknet Backbone             |
| YOLOv5  | 2020 | ~67.3%     | 140 | PyTorch Implementation, einfach |
| YOLOv8  | 2023 | ~70.0%     | 120 | Aktuell empfohlen               |

> **FÃ¼r Ãœbung 2 verwenden wir YOLOv8** â€” beste Balance zwischen Performance und Benutzerfreundlichkeit!


### YOLOv8 im Detail

YOLOv8 ist die aktuell empfohlene Version von Ultralytics. Sie bietet eine sehr einfache Python-API und eignet sich gut fÃ¼r Integration in ROS 2 und andere Anwendungen.

**Architektur-Ãœberblick:**

```ascii
YOLOv8 Architektur:

Input               Backbone           Neck              Head
640Ã—640Ã—3          (CSPDarknet)     (PAN-FPN)      (Detection Head)
â”Œâ”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”
â”‚     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚ â–“â–“â–“ â”‚ â”€â”€â”€â”€â”€â”€>  â”‚ â–“â–“  â”‚ â”€â”€â”€â”€â”€â”€>  â”‚BBox â”‚
â”‚ ðŸš¶  â”‚  Feature    â”‚ â–“â–“â–“ â”‚  Multi-  â”‚ â–“â–“  â”‚  Predict â”‚Classâ”‚
â”‚     â”‚  Extraction â”‚ â–“â–“â–“ â”‚  Scale   â”‚ â–“â–“  â”‚          â”‚Conf â”‚
â””â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”˜
                    ca. 20 Mio        Fusion          Outputs  
                    Parameter         Layers                                                               .
```

**YOLOv8 Modell-Varianten:**

<!-- data-type="none" -->
| Modell  | Parameteranzahl | mAP  | Speed (ms) | Typische Verwendung            |
| ------- | --------------- | ---- | ---------- | ------------------------------ |
| YOLOv8n | 3.2 Mio         | 37.3 | 1.2        | Embedded, Edge-Devices         |
| YOLOv8s | 11.2 Mio        | 44.9 | 1.9        | Gute Balance, schneller Laptop |
| YOLOv8m | 25.9 Mio        | 50.2 | 3.2        | Standardwahl                   |
| YOLOv8l | 43.7 Mio        | 52.9 | 4.5        | HÃ¶here Genauigkeit             |
| YOLOv8x | 68.2 Mio        | 53.9 | 6.8        | Beste Genauigkeit              |

> **Empfehlung fÃ¼r Ãœbung 2:** YOLOv8n oder YOLOv8s â€” schnell genug fÃ¼r Echtzeit auf Laptops!

!?[](https://www.youtube.com/watch?v=svn9-xV7wjk)

### YOLOv8 in der Anwendung

YOLOv8 ist die aktuell empfohlene Version von Ultralytics. Sie bietet eine sehr einfache Python-API und ist optimal fÃ¼r ROS 2 Integration geeignet.

**Installation und Nutzung:**

```python
# Installation
pip install ultralytics

# Import
from ultralytics import YOLO

# Vortrainiertes Modell laden (COCO-Dataset)
model = YOLO('yolov8n.pt')  # nano model
# Inferenz auf einem Bild
results = model('image.jpg')
# Oder auf Video/Webcam
results = model('video.mp4', stream=True)
```

**Output-Format verstehen:**

```python
# Ergebnisse iterieren
for result in results:
    boxes = result.boxes  # Bounding Boxes

    for box in boxes:
        # Koordinaten
        x1, y1, x2, y2 = box.xyxy[0]  # [x_min, y_min, x_max, y_max]
        # Oder: Center + Width/Height
        x_center, y_center, w, h = box.xywh[0]
        # Confidence Score
        confidence = box.conf[0]
        # Klassen-ID (COCO)
        class_id = int(box.cls[0])
        # Klassen-Name
        class_name = model.names[class_id]

        print(f"{class_name}: {confidence:.2f} at ({x1}, {y1})")
```

### COCO-Dataset und Personendetektion

    --{{0}}--
YOLOv8 wird standardmÃ¤ÃŸig auf dem COCO-Dataset trainiert. Dieses enthÃ¤lt 80 Objektklassen.

**COCO (Common Objects in Context):**

+ 80 Objektklassen
+ 330K Bilder
+ 1.5M Objektinstanzen
+ "Industry-Standard" fÃ¼r Object Detection

https://cocodataset.org/#home

**Wichtige Klassen (Auswahl):**

| Class ID | Name       | Class ID | Name       |
| -------- | ---------- | -------- | ---------- |
| 0        | **person** | 5        | bus        |
| 1        | bicycle    | 16       | dog        |
| 2        | car        | 17       | cat        |
| 3        | motorcycle | 62       | chair      |
| 4        | airplane   | 73       | book       |

**Personendetektion mit YOLOv8:**

```python
from ultralytics import YOLO

# Modell laden
model = YOLO('yolov8n.pt')

# Bild laden
image = cv2.imread('scene.jpg')

# Inferenz
results = model(image)

# Nur Personen filtern
persons = []
for box in results[0].boxes:
    if int(box.cls[0]) == 0:  # person class
        if box.conf[0] > 0.5:  # confidence threshold
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            persons.append({
                'bbox': [x1, y1, x2, y2],
                'confidence': float(box.conf[0]),
                'center': ((x1+x2)/2, (y1+y2)/2)  # Wichtig fÃ¼r Stereo!
            })

print(f"Detected {len(persons)} persons")
```

> Experimentieren Sie mit der Demo Implementierung im Ordner /yolo_Example.

### Von Bounding Box zu 3D-Position

    --{{0}}--
Dies ist der entscheidende Schritt fÃ¼r Ãœbung 2: Wir kombinieren YOLOv8-Detektionen mit der DisparitÃ¤tskarte aus der Stereo-Kamera!

**Pipeline:**

```ascii
Step 1: Detection        Step 2: Center Point      Step 3: Depth Lookup
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”  â”‚              â”‚    â—    â”‚              â”‚    â—    â”‚
â”‚  â”‚ ðŸš¶â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  (cx,cy)â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚  Z=3.2m â”‚
â”‚  â””â”€â”€â”€â”˜  â”‚  YOLOv8      â”‚         â”‚  Disparity   â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Map         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 4: 3D Coordinates
   (X, Y, Z) in camera frame
        â”‚
        v
   Transform to robot frame (TF2)                                                                         .
```

**Mathematik:**

> Aus L06 wissen wir: Die Tiefe (Entfernung) eines Punkts zur Kamera ist proportional zum Produkt aus Brennweite und Abstand zwischen den Kameras und umgekehrt proportional zur DisparitÃ¤t (dem Pixelversatz) dieses Punkts in den Stereo-Bildern.

$$
Z = \frac{f \cdot b}{d}
$$

Wobei:

+ $f$ ist die Brennweite der Kamera (in Pixeln).
+ $b$ ist die Baseline, also der Abstand zwischen den beiden Kameras.
+ $d$ ist die DisparitÃ¤t, also die Differenz der Position des Punkts in den zwei Bildern.

Nun nutzt du $Z$ und die Bildkoordinaten $(u, v)$, um die reale Position im Kamerakoordinatensystem zu berechnen:

$$
X = \frac{(u - c_x) \cdot Z}{f_x}
$$

$$
Y = \frac{(v - c_y) \cdot Z}{f_y}
$$

mit 

+ $c_x, c_y$ sind die Koordinaten des Hauptpunkts (Principal Point) in Pixeln
+ $f_x, f_y$ sind die Brennweiten in Pixeln entlang der $x$- bzw. $y$-Achse (aus der Kamerakalibrierung)

**Python-Implementierung:**

```python
def get_3d_position(bbox, disparity_map, camera_info):
    """
    Berechne 3D-Position aus Bounding Box und DisparitÃ¤tskarte

    Args:
        bbox: [x1, y1, x2, y2] in Pixel
        disparity_map: DisparitÃ¤tskarte (gleiche AuflÃ¶sung wie Bild)
        camera_info: Kamera-Kalibrierung (fx, fy, cx, cy, baseline)

    Returns:
        (X, Y, Z) in Metern (camera frame)
    """
    # Center der Bounding Box
    cx = int((bbox[0] + bbox[2]) / 2)
    cy = int((bbox[1] + bbox[3]) / 2)

    # DisparitÃ¤t an diesem Punkt
    disparity = disparity_map[cy, cx]

    # Tiefe berechnen (aus L06)
    if disparity > 0:
        Z = (camera_info['fx'] * camera_info['baseline']) / disparity
    else:
        return None  # Keine gÃ¼ltige Tiefe

    # 3D-Koordinaten
    X = (cx - camera_info['cx']) * Z / camera_info['fx']
    Y = (cy - camera_info['cy']) * Z / camera_info['fy']

    return (X, Y, Z)
```

**Robustheit verbessern:**

Problem: DisparitÃ¤t kann Rauschen enthalten oder ungÃ¼ltig sein

LÃ¶sungen:

```python
# 1. Verwende Median Ã¼ber mehrere Pixel
window_size = 5
cx_min = max(0, cx - window_size//2)
cx_max = min(disparity_map.shape[1], cx + window_size//2)
cy_min = max(0, cy - window_size//2)
cy_max = min(disparity_map.shape[0], cy + window_size//2)

window = disparity_map[cy_min:cy_max, cx_min:cx_max]
disparity = np.median(window[window > 0])  # Ignoriere 0-Werte

# 2. PlausibilitÃ¤ts-Check
if 0.5 < Z < 10.0:  # Personen zwischen 0.5m und 10m
    return (X, Y, Z)
else:
    return None
```

## Objekt Tracking

    --{{0}}--
Objekterkennung liefert uns Detektionen in einzelnen Bildern. Tracking verbindet diese Ã¼ber Zeit und ermÃ¶glicht uns, Bewegungen vorherzusagen und IDs zu erhalten.

**Vorteile gegenÃ¼ber reiner Detektion:**

+ **Temporal Coherence**: GlÃ¤ttung Ã¼ber Zeit reduziert Rauschen
+ **ID-Konsistenz**: "Person #1" bleibt "Person #1" Ã¼ber mehrere Frames
+ **PrÃ¤diktive Suche**: Effizienter durch EinschrÃ¤nkung des Suchbereichs
+ **Okklusion-Handling**: Objekte kÃ¶nnen kurzzeitig verdeckt werden
+ **Trajektorien**: Geschwindigkeit und Richtung bestimmen

**Anwendungen:**

+ Autonome Fahrzeuge: FuÃŸgÃ¤nger-Trajektorien vorhersagen
+ Person-Following Roboter: Verfolge spezifische Person
+ Videoanalyse: ZÃ¤hle Personen, die durch TÃ¼r gehen

### Optical Flow: Lucas-Kanade

    --{{0}}--
Optischer Fluss beschreibt die Bewegung von Pixeln zwischen zwei aufeinanderfolgenden Bildern.

**Annahmen:**

1. **Brightness Constancy**: Pixel-IntensitÃ¤t bleibt konstant
   $$I(x, y, t) = I(x + \Delta x, y + \Delta y, t + \Delta t)$$

2. **Small Motion**: Bewegung ist klein zwischen Frames

3. **Spatial Coherence**: Nachbar-Pixel bewegen sich Ã¤hnlich

**Grundannahme der Lucas-Kanade Methode:** Ein Pixel behÃ¤lt seine Helligkeit zwischen den Frames: $I(x, y, t) = I(x + u, y + v, t + 1)$

**Realisierung:**

| Gradient | Berechnung | Bedeutung |
|----------|------------|-----------|
| $I_x$ | Sobel in x-Richtung | HelligkeitsÃ¤nderung horizontal |
| $I_y$ | Sobel in y-Richtung | HelligkeitsÃ¤nderung vertikal |
| $I_t$ | Frame2 - Frame1 | HelligkeitsÃ¤nderung zeitlich |

Die Verschiebungen werden auf Basis der **Helligkeitsgradienten** berechnet - nicht durch Deskriptor-Matching wie bei ORB/SIFT.

```ascii 
Fenster um Feature-Punkt (z.B. 5Ã—5 Pixel):

Frame t:              Frame t+1:           Differenz (I_t):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 120 125 130 â”‚       â”‚ 118 123 128 â”‚      â”‚ -2  -2  -2  â”‚
â”‚ 140 200 145 â”‚  â†’    â”‚ 142 202 147 â”‚  =   â”‚  2   2   2  â”‚
â”‚ 135 130 125 â”‚       â”‚ 137 132 127 â”‚      â”‚  2   2   2  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  .
``` 

Aus den rÃ¤umlichen Gradienten ($I_x$, $I_y$) und der zeitlichen Ã„nderung ($I_t$) wird ein **Ã¼berbestimmtes Gleichungssystem** aufgestellt (eine Gleichung pro Pixel im Fenster):

**Beispiel fÃ¼r 2 Pixel:**

Pixel 1 an Position $(x_1, y_1)$: $\quad I_{x,1} \cdot u + I_{y,1} \cdot v = -I_{t,1}$

Pixel 2 an Position $(x_2, y_2)$: $\quad I_{x,2} \cdot u + I_{y,2} \cdot v = -I_{t,2}$

In Matrixform:

$$
\begin{bmatrix} I_{x,1} & I_{y,1} \\ I_{x,2} & I_{y,2} \end{bmatrix}
\begin{bmatrix} u \\ v \end{bmatrix} =
\begin{bmatrix} -I_{t,1} \\ -I_{t,2} \end{bmatrix}
$$

Bei einem 5Ã—5 Fenster haben wir **25 Gleichungen** fÃ¼r nur **2 Unbekannte** $(u, v)$ â†’ Ã¼berbestimmt! Die LÃ¶sung erfolgt per **Least Squares**.

**Unterschied zu Feature-Matching:**

| Lucas-Kanade | ORB/SIFT Matching |
|--------------|-------------------|
| Nutzt **Gradienten** (HelligkeitsÃ¤nderungen) | Nutzt **Deskriptoren** (Bit-Strings) |
| Sucht minimalen Helligkeitsfehler | Sucht minimale Hamming-Distanz |
| Braucht **kleine Bewegungen** | Funktioniert bei groÃŸen Verschiebungen |
| Sehr schnell (nur Matrixinversion) | Aufwendiger (alle Paare vergleichen) |

> Die Verschiebung wird also **nicht durch Suche nach Ã¤hnlichen Mustern** gefunden, sondern durch **LÃ¶sen einer Gleichung**, die beschreibt, wohin sich der Helligkeitsgradient bewegt haben muss.

### DeepSORT: Deep Simple Online Realtime Tracking

    --{{0}}--
DeepSORT ist der aktuelle Standard fÃ¼r Multi-Object Tracking. Er kombiniert Kalman-Filter mit Deep Learning Features.

**DeepSORT-Pipeline:**

```ascii
Frame t-1                     Frame t
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Person1 â”‚                  â”‚ Person? â”‚
â”‚ Person2 â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Person? â”‚
â”‚ Person3 â”‚  Predict & Match â”‚ Person? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                             â”‚
    v                             v
Kalman Filter Prediction    YOLO Detections
    â”‚                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               v
        Hungarian Algorithm
        (Data Association)
               â”‚
               v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Track 1 â”‚ â† Person1 matched
        â”‚ Track 2 â”‚ â† Person2 matched
        â”‚ Track 3 â”‚ â† Person3 lost (tentative)
        â”‚ Track 4 â”‚ â† New detection
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                                                    .
```

**Komponenten:**

**1. Detection (YOLO)** - Liefert Bounding Boxes pro Frame

**2. Kalman Filter**
   - PrÃ¤diziert Position im nÃ¤chsten Frame
   - State: $(x, y, a, h, \dot{x}, \dot{y}, \dot{a}, \dot{h})$

     - $(x, y)$ = Center
     - $a$ = Aspect Ratio
     - $h$ = HÃ¶he
     - Ableitungen = Geschwindigkeiten

**3. Appearance Descriptor**

   - CNN-Features fÃ¼r jede Detection
   - Hilft bei Re-Identification nach Okklusion
   - Typisch: 128-dim Feature Vector

**4. Hungarian Algorithm**

   - Optimale Zuordnung: Detection â†’ Track
   - Minimiert kombinierte Kostenfunktion:
     $$c_{i,j} = \lambda \cdot d_{\text{Mahalanobis}} + (1-\lambda) \cdot d_{\text{Cosine}}$$

**5. Track Management**

   - **Confirmed**: Track existiert Ã¼ber $n$ Frames
   - **Tentative**: Neue Detection, noch unsicher
   - **Deleted**: Track verloren Ã¼ber $m$ Frames

**Kostenfunktion im Detail:**

$$
d_{\text{Mahalanobis}} = \sqrt{(d_j - y_i)^T S_i^{-1} (d_j - y_i)}
$$

+ $d_j$ = Detection $j$
+ $y_i$ = Kalman-PrÃ¤diktion fÃ¼r Track $i$
+ $S_i$ = Kovarianzmatrix

$$
d_{\text{Cosine}} = 1 - \frac{r_j^T \cdot r_i^k}{||r_j|| \cdot ||r_i^k||}
$$

+ $r_j$ = Appearance Feature von Detection $j$
+ $r_i^k$ = Appearance Features von Track $i$ (letzten $k$ Frames)


## 3D-Objekterkennung in Punktwolken

Bisher haben wir 2D-Bilder betrachtet. FÃ¼r mobile Roboter sind Punktwolken aus Lidar oder Stereo-Kameras ebenso wichtig. Hier lernen wir grundlegende 3D-Algorithmen.

### RANSAC: Plane Segmentation

RANSAC (Random Sample Consensus) ist ein robuster Algorithmus zum Fitten von Modellen in verrauschten Daten.

+ Ebenen-Fitting â€“ z.B. Bodenerkennung in Punktwolken (wie im Beispiel)
+ Linien-Fitting â€“ z.B. Fahrspurerkennung in Bildern
+ Homographie-SchÃ¤tzung â€“ Transformation zwischen zwei Bildebenen
+ Fundamental-/Essential-Matrix â€“ Geometrische Beziehung zwischen Stereo-Bildern
+ 3D-Registrierung â€“ Ausrichtung von Punktwolken
+ Kreis-/Zylinder-Fitting â€“ Erkennung runder Objekte

![](https://www.open3d.org/docs/0.19.0/_images/tutorial_pipelines_global_registration_15_1.png "Beispiel fÃ¼r die Anwendung von RANSAC zur Ebenen-Segmentierung in einer Punktwolke aus der Dokumentation von Open3D")

**Problem**: Finde Boden-Ebene in Punktwolke

**Ebenen-Gleichung:**

$$
ax + by + cz + d = 0
$$

Normalisiert mit $\sqrt{a^2 + b^2 + c^2} = 1$

**RANSAC-Algorithmus:**

FÃ¼r k Iterationen:

  1. WÃ¤hle zufÃ¤llig 3 Punkte
  2. Berechne Ebene durch diese Punkte
  3. ZÃ¤hle Inliers (Punkte mit Distanz < threshold)
  4. Speichere beste LÃ¶sung

RÃ¼ckgabe: Ebene mit meisten Inliers 

RANSAC ist keine klassische kontinuierliche Optimierung (z.B. Gradient-Descent auf einer Differenzierbaren Kostenfunktion), sondern:

+ Stochastisch: Es probiert viele zufÃ¤llige Modell-Kandidaten aus.
+ Diskret: Bewertet Modelle nach Anzahl der Inlier (eine diskrete, nicht differenzierbare "Kostenfunktion").

**Mathematik:**

Ebene durch 3 Punkte $P_1, P_2, P_3$:

Normalenvektor:
$$
\vec{n} = (P_2 - P_1) \times (P_3 - P_1)
$$

Ebenengleichung:
$$
\vec{n} \cdot (P - P_1) = 0
$$

Distanz Punkt $P$ zur Ebene:
$$
d = \frac{|ax_P + by_P + cz_P + d|}{\sqrt{a^2 + b^2 + c^2}}
$$

Oft wird das beste Modell noch durch eine Least-Squares-Optimierung auf den gefundenen Inliern feinjustiert â€” das ist dann eine klassische kontinuierliche Optimierung, die Fehler quadratisch minimiert.

**Anwendungen:**

+ Boden-Entfernung fÃ¼r Navigation
+ Wand-Detektion
+ Tisch-OberflÃ¤chen finden (fÃ¼r Grasping)


### 3D Feature Descriptors: FPFH

Um Objekte in 3D zu erkennen, brauchen wir Deskriptoren - Ã¤hnlich wie ORB in 2D.

**FPFH (Fast Point Feature Histograms)**

Beschreibt die lokale Geometrie um einen Punkt herum.

**Berechnung:**

```ascii
Schritt 1: Normalen          Schritt 2: SPFH             Schritt 3: FPFH
                             (fÃ¼r Punkt p)               (Aggregation)

    â†— nâ‚                         p                           p
   Â·                            /|\                         /|\
  â†— nâ‚‚                         / | \                    â”€â”€â”€/â”€|â”€\â”€â”€â”€
 Â·    Â·                      qâ‚  qâ‚‚  qâ‚ƒ                  gewichtete
    â†— nâ‚ƒ                     Winkel Î±,Ï†,Î¸               Summe der
   Â·                         â†’ Histogramm                Nachbar-SPFHs                                      .
```

**Schritt 1: Surface Normals berechnen**

FÃ¼r jeden Punkt wird die lokale OberflÃ¤chenorientierung geschÃ¤tzt. Dazu wird eine Ebene durch die Nachbarpunkte gefittet (PCA oder Least Squares) - der Normalenvektor dieser Ebene ist die Surface Normal.

**Schritt 2: SPFH (Simplified Point Feature Histogram)**

FÃ¼r einen Punkt $p$ betrachten wir **alle** Nachbarn $q_1, q_2, ..., q_k$ im Radius $r$ (typisch: 20-50 Nachbarn, nicht nur 3!). FÃ¼r **jedes Paar** $(p, q_i)$ berechnen wir drei Winkel.

**Warum drei Winkel?** Die relative Lage zweier orientierter Punkte im 3D-Raum hat mehrere Freiheitsgrade:

Ein Winkel allein wÃ¼rde z.B. nicht unterscheiden, ob der Nachbar "links" oder "rechts" von $p$ liegt, oder ob die Normale "nach vorne" oder "zur Seite" zeigt. Die drei Winkel kodieren die vollstÃ¤ndige relative Orientierung in einem lokalen Koordinatensystem $(u, v, w)$, das an $p$ verankert ist.

```ascii
Beispiel: Punkt p hat 4 Nachbarn im Radius r

         qâ‚‚
          Â·
    qâ‚ Â·  p  Â· qâ‚ƒ        FÃ¼r jedes Paar (p,q) berechne Î±, Ï†, Î¸:
          Â·                 (p,qâ‚) â†’ Î±=0.2,  Ï†=0.8,  Î¸=1.5
         qâ‚„                 (p,qâ‚‚) â†’ Î±=0.7,  Ï†=0.3,  Î¸=0.9
                            (p,qâ‚ƒ) â†’ Î±=0.3,  Ï†=0.6,  Î¸=2.1
                            (p,qâ‚„) â†’ Î±=0.5,  Ï†=0.4,  Î¸=1.2                                                 .
```

Diese Winkelwerte werden nun in **drei separate Histogramme** einsortiert:

```ascii
Histogramm fÃ¼r Î±:          Histogramm fÃ¼r Ï†:          Histogramm fÃ¼r Î¸:

HÃ¤ufigkeit                 HÃ¤ufigkeit                 HÃ¤ufigkeit
    â”‚   â–“                      â”‚ â–“                        â”‚     â–“
    â”‚   â–“ â–“                    â”‚ â–“ â–“                      â”‚ â–“   â–“
    â”‚ â–“ â–“ â–“                    â”‚ â–“ â–“ â–“                    â”‚ â–“ â–“ â–“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bins          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bins          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bins
     1 2 3 ... 11               1 2 3 ... 11               1 2 3 ... 11                                   .
```

Die drei Histogramme werden konkateniert â†’ **33 Werte** (3 Ã— 11 Bins). Die Bin-Anzahl 11 ist empirisch gewÃ¤hlt (einstellbar in PCL).

**Schritt 3: FPFH (Fast Point Feature Histogram)**

Das "Fast" kommt daher, dass nicht alle Punktpaare betrachtet werden. Stattdessen wird der SPFH von $p$ mit den SPFHs seiner Nachbarn gewichtet kombiniert:

$$FPFH(p) = SPFH(p) + \frac{1}{k} \sum_{i=1}^{k} \frac{1}{d_i} \cdot SPFH(q_i)$$

NÃ¤here Nachbarn ($d_i$ klein) haben mehr Einfluss. So entsteht ein 33-dimensionaler Deskriptor, der die lokale 3D-Geometrie charakterisiert.

**Features:**

+ 33-dimensionaler Histogram-Descriptor
+ Rotation-invariant
+ Robust gegen Rauschen
+ Skaliert gut (schneller als PFH)

**Anwendung: Object Recognition**

```cpp
// Template Matching mit FPFH
// 1. Berechne FPFH fÃ¼r Template
// 2. Berechne FPFH fÃ¼r Szene
// 3. Finde korrespondierende Features (KNN)
// 4. RANSAC fÃ¼r robuste Transformation
// 5. ICP fÃ¼r Fine-Alignment
```

### ICP: Iterative Closest Point

    --{{0}}--
ICP ist der Standardalgorithmus fÃ¼r Point Cloud Registration - das Ausrichten zweier Punktwolken.

**Problem**: Gegeben zwei Punktwolken $P$ und $Q$, finde Transformation $T$, sodass $T(P) \approx Q$

**ICP-Algorithmus:**

```
Initialisierung: T = I (IdentitÃ¤t)

Wiederhole bis Konvergenz:
  1. FÃ¼r jeden Punkt p in P:
     - Finde nÃ¤chsten Punkt q in Q

  2. Berechne optimale Transformation T':
     - Minimiere Sum of Squared Distances

  3. Update: T = T' Â· T

  4. Wenn Ã„nderung < threshold: Stop
```

**Mathematik:**

Minimiere:
$$
E(R, t) = \sum_{i=1}^{N} || R p_i + t - q_i ||^2
$$

Wobei:
+ $R$ = Rotationsmatrix (3Ã—3)
+ $t$ = Translationsvektor (3Ã—1)

LÃ¶sung: **SVD (Singular Value Decomposition)**

**Anwendungen:**

+ Template Matching (Objekterkennung)
+ SLAM (Scan Matching)
+ 3D-Rekonstruktion
+ Pose Estimation

**Limitierungen:**

+ BenÃ¶tigt gute Initialisierung
+ Konvergiert zu lokalem Minimum
+ Langsam bei groÃŸen Punktwolken

**Verbesserungen:**

+ **Generalized ICP**: Verwendet Plane-to-Plane statt Point-to-Point
+ **Point-to-Plane ICP**: Verwendet Surface Normals
+ **Feature-based Registration**: Erst grobe Ausrichtung mit Features (FPFH), dann ICP

### Deep Learning fÃ¼r 3D: PointNet++

    --{{0}}--
Moderne AnsÃ¤tze verwenden neuronale Netze direkt auf Punktwolken - ohne Voxelisierung!

**PointNet (2017):**

Erste End-to-End Deep Learning auf Punktwolken

+ Input: $N \times 3$ Matrix (N Punkte mit x, y, z)
+ Permutation-invariant durch Max-Pooling
+ Klassifikation und Segmentation

**PointNet++ (2017):**

Hierarchische Architektur mit lokalem Kontext

```ascii
Input Points         Set Abstraction      Classification
  N Ã— 3               Layers              Output
â”Œâ”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Â· Â· Â·â”‚  â”€â”€â”€â”€â”€â”€â”€>  â”‚ â–“â–“â–“  â”‚  â”€â”€â”€â”€â”€â”€â”€>  â”‚ Car  â”‚
â”‚Â· Â· Â· â”‚  Group &   â”‚ â–“â–“   â”‚  MLP +     â”‚ Tree â”‚
â”‚ Â· Â· Â·â”‚  Sample    â”‚ â–“    â”‚  Pooling   â”‚ ...  â”‚
â””â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”˜                                                          .
```

**Vorteile:**

+ Direkte Verarbeitung von Punktwolken
+ Permutation-invariant
+ Robust gegen unterschiedliche Dichten

**Anwendungen:**

+ 3D Object Classification
+ 3D Object Detection (fÃ¼r autonome Fahrzeuge)
+ 3D Semantic Segmentation

**Aktuelle Forschung (2024):**

+ **Point Transformer**: Self-Attention fÃ¼r Punktwolken
+ **VoxelNet**: Hybrid aus Voxeln und PointNet
+ **BEVFusion**: Multi-Modal (Camera + Lidar) fÃ¼r autonome Fahrzeuge

> Deep Learning fÃ¼r 3D ist ein aktives Forschungsgebiet - die Methoden entwickeln sich rasant!

## Zusammenfassung & Ausblick

    --{{0}}--
Wir haben heute eine umfassende EinfÃ¼hrung in Objekterkennung und Tracking erhalten - von klassischen Feature-basierten Methoden bis zu modernem Deep Learning, von 2D-Bildern bis zu 3D-Punktwolken.

Die Vorlesung spannte einen Bogen von klassischen bis hin zu modernen Verfahren der Objekterkennung:

**Klassische Feature-Methoden** bilden die Grundlage fÃ¼r geometrische Anwendungen wie SLAM und visuelle Odometrie. Mit Harris und Shi-Tomasi haben wir Corner-Detektoren kennengelernt, die mathematisch fundiert arbeiten und ohne Training auskommen. ORB kombiniert schnelle Detektion (FAST) mit kompakten binÃ¤ren Deskriptoren (rBRIEF).

**Deep Learning** ermÃ¶glicht semantisches VerstÃ¤ndnis von Szenen. YOLOv8 erkennt Objekte in Echtzeit und liefert die Bounding Boxes, die wir in Ãœbung 2 mit Stereo-Tiefendaten kombinieren werden. Die Vortrainierung auf COCO macht den Einstieg einfach - gleichzeitig sollte man die Grenzen dieser "Black Box"-Modelle im Blick behalten.

**Tracking** erweitert die Einzelbild-Detektion um zeitliche Konsistenz. Der Lucas-Kanade Algorithmus verfolgt Features zwischen Frames durch Gradientenanalyse, wÃ¤hrend DeepSORT komplexe Szenen mit mehreren Objekten handhabt und dabei IDs Ã¼ber Verdeckungen hinweg erhÃ¤lt.

**3D-Verfahren** nutzen die Punktwolken aus Lidar oder Stereo-Kameras. RANSAC segmentiert robuste Ebenen (etwa den Boden), Euclidean Clustering separiert Objekte, und ICP aligniert Punktwolken - wichtig fÃ¼r Scan-Matching in SLAM. Mit PointNet++ haben wir einen Ausblick auf Deep Learning direkt auf 3D-Daten gegeben.


### Ausblick: NÃ¤chste Vorlesung

**L09: Sensordatenfusion I - Grundlagen**

+ Warum mehrere Sensoren kombinieren?
+ Diskrete Bayes-Filter
+ Fehlerfortpflanzung und Unsicherheiten
+ Vorbereitung auf Kalman-Filter