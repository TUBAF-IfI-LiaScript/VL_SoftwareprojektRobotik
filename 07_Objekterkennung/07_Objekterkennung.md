<!--

author:   Sebastian Zug & Claude.ai
email:    sebastian.zug@informatik.tu-freiberg.de & Georg.Jaeger@informatik.tu-freiberg.de
version:  0.0.1
language: de
narrator: Deutsch Female

import: https://github.com/liascript/CodeRunner
        https://raw.githubusercontent.com/liascript-templates/plantUML/master/README.md
        https://raw.githubusercontent.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/refs/heads/master/config.md

-->

[![LiaScript](https://raw.githubusercontent.com/LiaScript/LiaScript/master/badges/course.svg)](https://liascript.github.io/course/?https://raw.githubusercontent.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/refs/heads/master/07_Objekterkennung/07_Objekterkennung.md)

# Objekterkennung und Tracking

| Parameter            | Kursinformationen                                                                                                     |
| -------------------- | --------------------------------------------------------------------------------------------------------------------- |
| **Veranstaltung:**   | @config.lecture                                                                                                       |
| **Semester**         | @config.semester                                                                                                      |
| **Hochschule:**      | `Technische Universit√§t Freiberg`                                                                                     |
| **Inhalte:**         | `Objekterkennung mit Features und Deep Learning, Tracking, 3D-Objekterkennung`                                        |
| **Link auf GitHub:** | https://github.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/blob/master/07_Objekterkennung/07_Objekterkennung.md |
| **Autoren**          | @author                                                                                                               |

![](https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif)

--------------------------------------------------------------------------------

**Zielstellung der heutigen Veranstaltung**

+ Verst√§ndnis feature-basierter Objekterkennung (ORB, AKAZE)
+ Deep Learning f√ºr Object Detection (YOLOv8) - **Vorbereitung f√ºr √úbung 2**
+ Object Tracking √ºber Bildsequenzen (DeepSORT)
+ 3D-Objekterkennung in Punktwolken (RANSAC, Clustering)
+ Integration in ROS 2 mit vision_msgs
+ **Praktische Anwendung: Personendetektion mit YOLOv8 + Stereo-Positionssch√§tzung**

--------------------------------------------------------------------------------

## Motivation: Warum Objekterkennung?

    --{{0}}--
In den letzten beiden Vorlesungen haben wir gelernt, wie Kameras Bilder aufnehmen und wie wir aus Stereo-Bildern Tiefeninformationen gewinnen k√∂nnen. Heute geht es um die n√§chste Stufe: Wie erkennen Roboter konkrete Objekte in ihrer Umgebung?

> Wir m√∂chten nicht nur sehen, sondern auch verstehen. Das bedeutet, dass wir Objekte identifizieren, lokalisieren und ihr Verhalten vorhersagen m√ºssen, um darauf reagieren zu k√∂nnen.

```ascii
Kamerabild          Objekt        Verhalten
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ ‚ñë‚ñë‚ñì ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ  üö∂ ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ Stop ‚îÇ
   ‚îÇ ‚ñì‚ñì‚ñë ‚îÇ         ‚îÇ @3m ‚îÇ        ‚îÇ  !   ‚îÇ
   ‚îÇ ‚ñë‚ñì‚ñì ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                                                         .
```

    --{{0}}--
Ein Roboter muss nicht nur Pixel sehen, sondern verstehen: Was ist das? Wo ist es? Wie bewegt es sich? Diese F√§higkeiten sind essentiell f√ºr autonome Navigation, Manipulation und Mensch-Roboter-Interaktion.

**Anwendungsbeispiele**

| Anwendung                     | Aufgabe                              | Beispiel                                   |
| ----------------------------- | ------------------------------------ | ------------------------------------------ |
| **Autonomes Fahren**          | Fu√üg√§nger, Fahrzeuge, Schilder       | Tesla Autopilot, Waymo                      |
| **Mobile Robotik**            | Hinderniserkennung, Person-Following | Serviceroboter in Hotels                    |
| **Industrielle Manipulation** | Objekterkennung f√ºr Pick-and-Place   | Bin-Picking in Lagerhallen                  |
| **Drohnen**                   | Landing Pad Detection                | Autonome Landung                            |
| **Soziale Robotik**           | Gesichtserkennung, Gestenerkennung   | Pepper, NAO                                 |

https://github.com/TUBAF-IfI-LiaScript/VL_SoftwareprojektRobotik/blob/master/00_Einfuehrung/images/ROSE2024_Chemnitz.pdf

> **√úbung 2 Kontext**: Wir werden YOLOv8 verwenden, um Personen in Kamerabildern zu erkennen und deren 3D-Position mit Stereo-Vision zu bestimmen!

### Begriffsdefinitionen

**Objektbezogene Perspective:**

| Begriff                              | Fragestellung               | Ergebnis                  | Beispiel                                       |
| ------------------------------------ | --------------------------- | ------------------------- | ---------------------------------------------- |
| **Detection** (Detektion)            | Wo ist ein Objekt?          | Bounding Box + Klasse     | "Person bei (320, 240), 80√ó150 Pixel"          |
| **Classification** (Klassifikation)  | Was ist das Objekt?         | Klasse + Confidence       | "Person mit 95% Wahrscheinlichkeit"            |
| **Identification** (Identifikation)  | Wer/welches Individuum?     | ID innerhalb einer Klasse | "Das ist Max M√ºller"                           |
| **Tracking** (Verfolgung)            | Wie bewegt sich das Objekt? | Trajektorie √ºber Zeit     | "Person #42 bewegt sich mit 1 m/s nach rechts" |

```ascii
Klassifikation vs. Identifikation:

Klassifikation:                    Identifikation:
"Was ist es?" (Kategorie)          "Wer ist es?" (Individuum)

  üö∂  üö∂  üö∂                         üö∂  üö∂  üö∂
   ‚Üì   ‚Üì   ‚Üì                          ‚Üì   ‚Üì   ‚Üì
 Person Person Person               Max  Anna  Tom
 (alle gleiche Klasse)              (unterschiedliche IDs)                                                  .
```

**Pixelbasierte Perspektive (Segmentation):**

| Begriff                   | Fragestellung                           | Ergebnis       | Beispiel               |
| ------------------------- | --------------------------------------- | -------------- | ---------------------- |
| **Instance Segmentation** | Welche Pixel geh√∂ren zu welchem Objekt? | Maske pro Objekt | Mask R-CNN           |
| **Semantic Segmentation** | Welcher Klasse geh√∂rt jedes Pixel?      | Klassenmaske   | Stra√üe, Gehweg, Himmel |



## Abgrenzung: Traditionelle vs. Deep Learning Methoden

In dieser Vorlesung behandeln wir **beide Ans√§tze** der Objekterkennung:

| Aspekt | Traditionelle Methoden (Features) | Deep Learning (CNN, YOLO) |
|--------|-----------------------------------|---------------------------|
| **Ziel** | Geometrische Korrespondenzen | Semantische Klassifikation |
| **Anwendung** | SLAM, Visual Odometry, Stereo-Matching, Kamerakalibrierung | Objekterkennung, Autonomes Fahren |
| **Trainingsdaten** | Keine n√∂tig | Gro√üe annotierte Datens√§tze |
| **Rechenleistung** | CPU ausreichend | GPU erforderlich |
| **Interpretierbarkeit** | Transparent, mathematisch fundiert | "Black Box" |

**Warum beides lernen?**

1. **Komplement√§re St√§rken**: Feature-Methoden liefern pr√§zise geometrische Information (wo ist ein Punkt im 3D-Raum?), w√§hrend Deep Learning semantisches Verst√§ndnis bietet (was ist das Objekt?).

2. **Hybride Systeme**: Moderne Robotik-Anwendungen kombinieren oft beide Ans√§tze - z.B. YOLO f√ºr Objekterkennung + ORB-Features f√ºr Tracking und Lokalisierung.

3. **Ressourcen-Constraints**: Auf eingebetteten Systemen sind klassische Methoden oft die einzige Option.

4. **Grundlagenverst√§ndnis**: Die mathematischen Konzepte hinter Feature-Detection bilden die Basis f√ºr das Verst√§ndnis moderner Architekturen.

## Feature-basierte Objekterkennung (2D)

    --{{0}}--
Traditionelle Objekterkennung basiert auf charakteristischen Merkmalen - sogenannten Features. Diese Methoden sind auch heute noch relevant, besonders f√ºr SLAM und visuelle Odometrie.

**Feature = charakteristischer, wiedererkennbarer Ausschnitt eines Bildes**

Eigenschaften guter Features:

+ **Repeatability**: Unter verschiedenen Bedingungen wiedererkennbar
+ **Distinctiveness**: Eindeutig unterscheidbar von anderen Features
+ **Locality**: Robust gegen Verdeckung
+ **Efficiency**: Schnell berechenbar
+ **Invariance**: Unabh√§ngig von Rotation, Skalierung, Beleuchtung

| Gute Features | Schlechte Features    |
| ------------- | --------------------- |
| Ecken ‚úì       | Glatte Fl√§chen ‚úó      |
| Kanten ‚úì      | Regelm√§√üige Muster ‚úó  |
| Blobs ‚úì       | Homogene Bereiche ‚úó   |

> Warum wollen wir √ºberhaupt gleiche Features in verschiedenen Bildern finden? Weil wir so Korrespondenzen herstellen k√∂nnen, die f√ºr 3D-Rekonstruktion, Bewegungssch√§tzung und Objekterkennung essentiell sind!

### Corner Detection: Harris & Shi-Tomasi & FAST

    --{{0}}--
Ecken sind ideale Features, weil sie in zwei Richtungen starke Gradienten haben.

**Harris Corner Detector (1988)**

Idee: Suche Bereiche, wo das Bild in alle Richtungen stark variiert

**Schritt 1: Gradientenbilder berechnen**

F√ºr jeden Pixel wird der Gradient (Helligkeits√§nderung) durch Faltung des Graustufenbildes $I$ mit dem Sobel-Kernel berechnet:

$$
I_x = I * S_x \quad \text{mit} \quad S_x = \begin{bmatrix} -1 & 0 & +1 \\ -2 & 0 & +2 \\ -1 & 0 & +1 \end{bmatrix}
$$

$$
I_y = I * S_y \quad \text{mit} \quad S_y = \begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ +1 & +2 & +1 \end{bmatrix}
$$

Dabei ist $I$ das Eingabebild (Graustufenwerte 0-255), $*$ die Faltungsoperation und $S_x$, $S_y$ die Sobel-Kernel. Das Ergebnis sind zwei komplette Bilder $I_x$ und $I_y$ mit den Gradienten f√ºr jeden Pixel.

**Schritt 2: Struktur-Matrix M f√ºr jeden Pixel**

F√ºr jeden Pixel $(x_0, y_0)$ wird √ºber ein lokales Fenster W summiert:

$$
M = \sum_{(x,y) \in W} \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix}
$$

```ascii
Bedeutung der Matrix-Eintr√§ge:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Œ£ Ix¬≤           ‚îÇ St√§rke horizontaler √Ñnderungen     ‚îÇ
‚îÇ Œ£ Iy¬≤           ‚îÇ St√§rke vertikaler √Ñnderungen       ‚îÇ
‚îÇ Œ£ Ix¬∑Iy         ‚îÇ Korrelation beider Richtungen      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Beispiele:

Vertikale Kante:        Horizontale Kante:      Ecke:
    ‚ñë‚ñë‚ñà‚ñà                    ‚ñë‚ñë‚ñë‚ñë                  ‚ñë‚ñë‚ñà‚ñà
    ‚ñë‚ñë‚ñà‚ñà                    ‚ñà‚ñà‚ñà‚ñà                  ‚ñà‚ñà‚ñà‚ñà

Ix: gro√ü, Iy: klein     Ix: klein, Iy: gro√ü    Ix: gro√ü, Iy: gro√ü
‚Üí Nur eine Richtung     ‚Üí Nur eine Richtung    ‚Üí Beide Richtungen!
‚Üí KANTE                 ‚Üí KANTE                ‚Üí ECKE ‚úì                                                     .
```

> Wie k√∂nnen wir aber die "St√§rke der Ecke" beschreiben? Daf√ºr nutzen wir die Eigenwerte der Matrix M.

**Schritt 3: Corner Response Function**

Die Eigenwerte $\lambda_1$, $\lambda_2$ von M beschreiben die **Hauptrichtungen und St√§rken der Intensit√§ts√§nderung**. Geometrisch definiert M eine Ellipse, deren Halbachsen durch die Eigenwerte gegeben sind:

```ascii
Flache Region:         Kante:                 Ecke:
(beide Œª klein)        (ein Œª gro√ü)           (beide Œª gro√ü)

      ¬∑                    |                    ‚îÄ‚îº‚îÄ
    ¬∑ ¬∑ ¬∑                  |                    ‚îÄ‚îº‚îÄ
      ¬∑                    |                    ‚îÄ‚îº‚îÄ

Ellipse: winzig        Ellipse: lang/schmal   Ellipse: kreisf√∂rmig
Œª‚ÇÅ ‚âà Œª‚ÇÇ ‚âà 0            Œª‚ÇÅ >> Œª‚ÇÇ               Œª‚ÇÅ ‚âà Œª‚ÇÇ >> 0                                                  .
```

| Situation | $\lambda_1$ | $\lambda_2$ | Bedeutung |
|-----------|-------------|-------------|-----------|
| **Flach** | klein | klein | Keine √Ñnderung in beide Richtungen |
| **Kante** | gro√ü | klein | Starke √Ñnderung nur senkrecht zur Kante |
| **Ecke** | gro√ü | gro√ü | Starke √Ñnderung in **beide** Richtungen |

Aber ... Harris suchte eine L√∂sung ohne Eigenwertberechnung! Harris nutzt einen Trick - Determinante und Spur kodieren die Eigenwerte:

$$
R = \det(M) - k \cdot \text{trace}(M)^2
$$

Mit $k \approx 0.04-0.06$

**Warum keine Eigenwertberechnung?**

1. Numerische Stabilit√§t: Die Wurzelberechnung kann bei sehr kleinen oder sehr √§hnlichen Eigenwerten zu numerischen Problemen f√ºhren (Division durch kleine Zahlen, Rundungsfehler)
2. Ausreichend f√ºr die Aufgabe: Harris braucht keine exakten Eigenwerte - er braucht nur eine Entscheidungsfunktion die sagt: "Ecke oder nicht". Die Kombination $\det(M) - k \cdot \text{trace}(M)^2$ liefert genau das, ohne die Eigenwerte explizit zu kennen.
3. Historisch (1988): Damals war die Rechenperformance tats√§chlich noch ein relevanter Faktor.

> Der Harris-Detektor ist rotationsinvariant, weil Determinante und Spur bei Rotation der Matrix erhalten bleiben:

**Shi-Tomasi (1994) - "Good Features to Track"**

Verbesserte Version: Verwendet direkt die kleinere Eigenwerte

$$
R = \min(\lambda_1, \lambda_2)
$$

Wenn $R > \text{threshold}$: Guter Feature-Punkt

**Visualisierung:**

> Diese Corner-Detektoren werden in SLAM-Systemen f√ºr Feature-Tracking verwendet!


```python -loadImage.py 
import cv2
import numpy as np

import urllib.request

def load_image_from_url(url):
    resp = urllib.request.urlopen(url)
    image_data = resp.read()
    image_array = np.asarray(bytearray(image_data), dtype=np.uint8)
    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
    return image
```
```python corner_detection.py
import cv2
import numpy as np
from loadImage import load_image_from_url

image = load_image_from_url('https://r4r.informatik.tu-freiberg.de/content/images/size/w960/2022/08/karl_kegel_bau1.jpg')

# Graustufenkonvertierung (notwendig f√ºr Corner Detection)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Harris Corner Detector
harris_response = cv2.cornerHarris(np.float32(gray), blockSize=2, ksize=3, k=0.04)

# Graustufenbild in BGR konvertieren f√ºr farbige Markierungen
image_harris = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
image_harris[harris_response > 0.01 * harris_response.max()] = [0, 0, 255]

# Shi-Tomasi Corner Detector ("Good Features to Track")
corners = cv2.goodFeaturesToTrack(gray, maxCorners=100, qualityLevel=0.01, minDistance=10)

# Graustufenbild in BGR konvertieren f√ºr farbige Markierungen
image_shi_tomasi = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
if corners is not None:
    for corner in corners:
        x, y = corner.ravel()
        cv2.circle(image_shi_tomasi, (int(x), int(y)), 5, (0, 255, 0), -1)

cv2.imwrite('harris_corners.png', image_harris)
cv2.imwrite('shi_tomasi_corners.png', image_shi_tomasi)
```
@LIA.eval(`["loadImage.py", "main.py"]`, `none`, `python3 main.py`, `*`)

> Immer noch zu viel Rechenaufwand f√ºr den Einsatz auf mobilen Robotern? Dann schauen wir uns jetzt einen extrem schnellen Detektor an: FAST!

**FAST (Features from Accelerated Segment Test)**

1. W√§hle einen Pixel $p$ mit Intensit√§t $I_p$
2. Betrachte 16 Pixel auf einem Kreis (Radius 3) um $p$
3. Ist $p$ eine Ecke, wenn $n$ aufeinanderfolgende Pixel heller/dunkler sind
4. Typisch: $n = 12$, Threshold $t = 20$

![](https://docs.opencv.org/3.4/fast_speedtest.jpg "OpenCV FAST Example")

Jedem der 16 Pixel wird ein Label zugewiesen:

+ Heller ($I_{kreis} > I_p + t$)
+ Dunkler ($I_{kreis}< I_p - t$)
+ √Ñhnlich (weder noch)

Dann wird gepr√ºft: Gibt es 12 zusammenh√§ngende Pixel, die alle dasselbe Label (heller oder dunkler) haben? Falls ja ‚Üí Ecke erkannt.

### Feature Descriptoren: Rotated BRIEF

> Nachdem wir nun Ecken detektieren k√∂nnen, brauchen wir eine M√∂glichkeit, diese Ecken zu beschreiben, damit wir sie in verschiedenen Bildern wiedererkennen k√∂nnen. Hierf√ºr verwenden wir Deskriptoren.

**BRIEF (Binary Robust Independent Elementary Features)**

Beschreibt eine Ecke durch Bin√§rvergleiche:

1. W√§hle $n$ Pixelpaare zuf√§llig um die Ecke
2. Vergleiche deren Intensit√§ten
3. Speichere Ergebnis als Bit-String

$$
\text{BRIEF}(p) = \sum_{1 \leq i \leq n} 2^{i-1} \cdot \tau(p; x_i, y_i)
$$

Wobei:
$$
\tau(p; x, y) = \begin{cases} 1 & \text{wenn } I(p_x) < I(p_y) \\ 0 & \text{sonst} \end{cases}
$$

Typisch: $n = 256$ ‚Üí 256-Bit-Deskriptor

https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf

> **Kernidee**: BRIEF vergleicht Intensit√§ten von Pixelpaaren und speichert das Ergebnis als Bit. Zwei Deskriptoren werden durch die Hamming-Distanz (Anzahl unterschiedlicher Bits) verglichen - sehr schnell durch XOR-Operation!

**Limitierung von BRIEF:** Nicht rotationsinvariant!

Das folgende Beispiel zeigt, wie BRIEF funktioniert: F√ºr einen Feature-Punkt werden zuf√§llige Pixelpaare verglichen und das Ergebnis als Bit-String gespeichert.

```python -loadImage.py
import cv2
import numpy as np
import urllib.request

def load_image_from_url(url):
    resp = urllib.request.urlopen(url)
    image_data = resp.read()
    image_array = np.asarray(bytearray(image_data), dtype=np.uint8)
    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
    return image
```
```python brief_demo.py
import cv2
import numpy as np
from loadImage import load_image_from_url

# Bild laden und in Graustufen konvertieren
image = load_image_from_url('https://r4r.informatik.tu-freiberg.de/content/images/size/w960/2022/08/karl_kegel_bau1.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (5, 5), 0)  # Gl√§ttung f√ºr Stabilit√§t

# Feature-Punkt (Zentrum f√ºr Demo)
feature_point = (gray.shape[1] // 2, gray.shape[0] // 2)
patch_size = 31  # Typische Patch-Gr√∂√üe f√ºr BRIEF

# Generiere zuf√§llige Pixelpaare (vereinfachte BRIEF-Variante)
np.random.seed(42)  # Reproduzierbarkeit
n_pairs = 16  # Reduziert f√ºr Visualisierung (normal: 256)
pairs = np.random.randint(-patch_size//2, patch_size//2, size=(n_pairs, 4))

# Berechne BRIEF-Deskriptor
def compute_brief(img, point, pairs):
    """Berechne einen vereinfachten BRIEF-Deskriptor"""
    descriptor = []
    px, py = point

    for i, (dx1, dy1, dx2, dy2) in enumerate(pairs):
        # Koordinaten der beiden Pixel im Paar
        x1, y1 = px + dx1, py + dy1
        x2, y2 = px + dx2, py + dy2

        # Bounds-Check
        if (0 <= x1 < img.shape[1] and 0 <= y1 < img.shape[0] and
            0 <= x2 < img.shape[1] and 0 <= y2 < img.shape[0]):
            # Vergleich: I(p1) < I(p2) => 1, sonst 0
            bit = 1 if img[y1, x1] < img[y2, x2] else 0
            descriptor.append(bit)
        else:
            descriptor.append(0)

    return descriptor

# Berechne Deskriptor
descriptor = compute_brief(gray, feature_point, pairs)

# Visualisierung
vis_image = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
px, py = feature_point

# Zeichne Feature-Punkt
cv2.circle(vis_image, (px, py), 8, (0, 255, 0), 2)

# Zeichne einige Pixelpaare
colors = [(255, 0, 0), (0, 0, 255)]  # Blau = 0, Rot = 1
for i, (dx1, dy1, dx2, dy2) in enumerate(pairs[:8]):  # Nur erste 8 f√ºr √úbersichtlichkeit
    x1, y1 = px + dx1, py + dy1
    x2, y2 = px + dx2, py + dy2
    color = colors[descriptor[i]]
    cv2.line(vis_image, (x1, y1), (x2, y2), color, 1)
    cv2.circle(vis_image, (x1, y1), 3, color, -1)
    cv2.circle(vis_image, (x2, y2), 3, color, -1)

# Ergebnis ausgeben
print("BRIEF-Deskriptor Demonstration")
print("=" * 40)
print(f"Feature-Punkt: ({px}, {py})")
print(f"Anzahl Bit-Vergleiche: {n_pairs}")
print(f"\nBin√§rer Deskriptor: {''.join(map(str, descriptor))}")
print(f"Als Integer: {int(''.join(map(str, descriptor)), 2)}")
print(f"\nVergleich zweier Deskriptoren (Hamming-Distanz):")

# Demonstriere Hamming-Distanz
descriptor2 = compute_brief(gray, (px + 5, py + 5), pairs)  # Leicht verschoben
hamming = sum(a != b for a, b in zip(descriptor, descriptor2))
print(f"Deskriptor 1: {''.join(map(str, descriptor))}")
print(f"Deskriptor 2: {''.join(map(str, descriptor2))}")
print(f"Hamming-Distanz: {hamming} Bits unterschiedlich")

cv2.imwrite('brief_visualization.png', vis_image)
print("\nVisualisierung gespeichert: brief_visualization.png")
print("(Blaue Linien = Bit 0, Rote Linien = Bit 1)")
```
@LIA.eval(`["loadImage.py", "main.py"]`, `none`, `python3 main.py`, `*`)

**Gibt es auch eine rotationsinvariante Umsetzung?**

ORB (Oriented FAST and Rotated BRIEF) ist die rotationsinvariante Variante von BRIEF:


1. Orientierung berechnen: F√ºr jeden Keypoint wird die dominante Orientierung mittels Intensity Centroid bestimmt:
$$\theta = \arctan2(m_{01}, m_{10})$$ wobei $m_{01}$ und $m_{10}$ die Bildmomente im Patch sind.

> **Was sind Bildmomente?** Die Bildmomente beschreiben den "Schwerpunkt der Intensit√§t" in einem kreisf√∂rmigen Patch (typisch Radius $r = 15$) um den Keypoint:
>
> ```ascii
> FAST-Detektion:              Moment-Berechnung:
>
>      ¬∑ ¬∑ ¬∑ ¬∑ ¬∑                  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
>     ¬∑         ¬∑                ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
>    ¬∑           ¬∑              ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
>    ¬∑     ‚óè     ¬∑      ‚Üí       ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚óè‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
>    ¬∑           ¬∑              ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
>     ¬∑         ¬∑                ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
>      ¬∑ ¬∑ ¬∑ ¬∑ ¬∑                  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
>
>    16 Pixel (r=3)             Voller Patch (r=15)
>    f√ºr Corner-Test            f√ºr Orientierung                                                            .
> ```
>
> **Analogie zur Mechanik:** Der Intensity Centroid entspricht dem Schwerpunkt eines K√∂rpers, wobei Pixelintensit√§ten die Rolle der Massen √ºbernehmen:
>
> | Mechanik | Bildverarbeitung |
> |----------|------------------|
> | Masse $m_i$ an Position $(x_i, y_i)$ | Intensit√§t $I(x,y)$ an Pixel $(x,y)$ |
> | Gesamtmasse $M = \sum m_i$ | $m_{00} = \sum I(x,y)$ |
> | Schwerpunkt $\bar{x} = \frac{\sum m_i x_i}{M}$ | Centroid $C_x = \frac{m_{10}}{m_{00}}$ |

2. Rotierte Pixelpaare: Die vordefinierten BRIEF-Pixelpaare werden entsprechend der Orientierung $\theta$ rotiert, bevor der Deskriptor berechnet wird.
3. Berechnung des Deskriptors: Wie bei BRIEF, aber mit den rotierten Pixelpaaren.

**Weitere Deskriptoren (Float-basiert):**

Neben bin√§ren Deskriptoren wie ORB/BRIEF gibt es Float-Deskriptoren, die kontinuierliche Werte speichern:

<!-- data-type="none" -->
| Deskriptor | Dimension | Prinzip | Besonderheit |
|------------|-----------|---------|--------------|
| **SIFT** | 128 floats | Histogramme der Gradientenrichtungen in 4√ó4 Subregionen | Skalen- und rotationsinvariant, patentiert bis 2020 |
| **SURF** | 64 floats | Haar-Wavelet-Antworten in Subregionen | Schneller als SIFT, √§hnliche Qualit√§t |
| **AKAZE** | variabel | Nichtlineare Skalierungsr√§ume mit Modified Local Difference Binary | Open-source Alternative zu SIFT |

> **Wann welchen Deskriptor?**
>
> - **ORB/BRIEF**: Echtzeitanwendungen (SLAM, Tracking) - schnell durch Hamming-Distanz
> - **SIFT/SURF**: Wenn Genauigkeit wichtiger als Geschwindigkeit ist (Panorama-Stitching, 3D-Rekonstruktion)
> - **AKAZE**: Guter Kompromiss - robust und frei verf√ºgbar

### Feature Matching

    --{{0}}--
Nachdem wir Features in zwei Bildern gefunden haben, m√ºssen wir korrespondierende Punkte finden.

**Brute-Force Matcher**

Einfachster Ansatz: Vergleiche jeden Deskriptor mit jedem anderen

+ F√ºr bin√§re Deskriptoren (ORB, BRIEF): **Hamming-Distanz**
+ F√ºr Float-Deskriptoren (SIFT, SURF): **Euklidische Distanz**

Hamming-Distanz = Anzahl unterschiedlicher Bits

```python
# Brute-Force Matcher f√ºr ORB (Hamming)
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(descriptors1, descriptors2)

# Sortiere nach Distanz
matches = sorted(matches, key=lambda x: x.distance)
```

**Interaktives Beispiel: √Ñhnlichste Ecken mit ORB finden**

Das folgende Beispiel findet alle ORB-Features in einem Bild und sucht die √§hnlichsten Ecken basierend auf der Hamming-Distanz ihrer Deskriptoren. Die Implementierung:

1. **Detektion**: ORB extrahiert bis zu 100 Keypoints mit FAST-Detektor
2. **Deskription**: F√ºr jeden Keypoint wird ein 256-Bit BRIEF-Deskriptor berechnet (32 Bytes)
3. **Matching**: Alle Paare werden verglichen - die Hamming-Distanz z√§hlt unterschiedliche Bits
4. **Visualisierung**: Die 10 √§hnlichsten Paare (kleinste Distanz) werden gr√ºn markiert

> **Beobachtung:** √Ñhnliche Deskriptoren bedeuten √§hnliche lokale Textur - nicht zwingend semantisch gleiche Objekte. Fensterecken links und rechts haben oft √§hnliche Gradienten und werden daher als "√§hnlich" erkannt.

```python -loadImage.py
import cv2
import numpy as np
import urllib.request

def load_image_from_url(url):
    resp = urllib.request.urlopen(url)
    image_data = resp.read()
    image_array = np.asarray(bytearray(image_data), dtype=np.uint8)
    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
    return image
```
```python orb_similar_corners.py
import cv2
import numpy as np
from loadImage import load_image_from_url

# Bild laden
image = load_image_from_url('https://r4r.informatik.tu-freiberg.de/content/images/size/w960/2022/08/karl_kegel_bau1.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# ORB-Detektor erstellen
orb = cv2.ORB_create(nfeatures=100)

# Keypoints und Deskriptoren berechnen
keypoints, descriptors = orb.detectAndCompute(gray, None)
print(f"Gefundene Keypoints: {len(keypoints)}")
print(f"Deskriptor-Shape: {descriptors.shape} (100 Keypoints x 32 Bytes = 256 Bits)")

# Alle Paare mit Hamming-Distanz berechnen
pairs = []
for i in range(len(descriptors)):
    for j in range(i + 1, len(descriptors)):
        distance = cv2.norm(descriptors[i], descriptors[j], cv2.NORM_HAMMING)
        pairs.append((distance, i, j))

# Nach Distanz sortieren und die 10 besten ausw√§hlen
pairs.sort(key=lambda x: x[0])
best_pairs = pairs[:10]

# Indizes der Keypoints in den besten Paaren sammeln
best_indices = set()
for dist, i, j in best_pairs:
    best_indices.add(i)
    best_indices.add(j)

print(f"\nDie 10 √§hnlichsten Paare:")
for rank, (dist, i, j) in enumerate(best_pairs, 1):
    kp_i, kp_j = keypoints[i], keypoints[j]
    similarity = 100 * (1 - dist/256)
    print(f"  {rank}. KP {i} ({kp_i.pt[0]:.0f},{kp_i.pt[1]:.0f}) <-> "
          f"KP {j} ({kp_j.pt[0]:.0f},{kp_j.pt[1]:.0f}): "
          f"Distanz={int(dist)}, √Ñhnlichkeit={similarity:.1f}%")

# Visualisierung
vis_image = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)

# Alle Keypoints in Grau zeichnen
for i, kp in enumerate(keypoints):
    if i not in best_indices:
        cv2.circle(vis_image, (int(kp.pt[0]), int(kp.pt[1])), 4, (128, 128, 128), 1)

# Die Keypoints der 10 besten Paare in Gr√ºn hervorheben
for idx in best_indices:
    kp = keypoints[idx]
    cv2.circle(vis_image, (int(kp.pt[0]), int(kp.pt[1])), 8, (0, 255, 0), 2)

cv2.imwrite('orb_similar_corners.png', vis_image)
print("\nVisualisierung gespeichert: orb_similar_corners.png")
print("(Gr√ºne Kreise = Keypoints der 10 √§hnlichsten Paare)")
```
@LIA.eval(`["loadImage.py", "main.py"]`, `none`, `python3 main.py`, `*`)

> **Beobachtung:** Die √§hnlichsten Ecken haben oft eine √§hnliche lokale Struktur - z.B. zwei Fensterecken oder zwei Geb√§udekanten mit √§hnlicher Textur.

**RANSAC f√ºr robuste Geometrie**

RANSAC folgt auf das Deskriptor-Matching und wirkt als geometrischer Konsistenzfilter auf die Matches.

Entferne Outliers durch geometrische Konsistenz auf der Basis von Essenzial- oder Homographiemodellen:

1. W√§hle zuf√§llig minimale Menge von Matches
2. Berechne Homographie $H$ (oder Fundamentalmatrix $F$)
3. Z√§hle Inliers (Matches konsistent mit $H$)
4. Wiederhole und w√§hle beste L√∂sung

```python
# RANSAC f√ºr Homographie
H, mask = cv2.findHomography(
    src_pts, dst_pts,
    cv2.RANSAC,
    ransacReprojThreshold=5.0
)

# mask[i] == 1: Inlier, mask[i] == 0: Outlier
inliers = src_pts[mask.ravel() == 1]
```

> Wir varieren die Modellparameter und suchen nach der besten L√∂sung mit den meisten Inliers! RANSAC arbeitet nicht auf den Des

!?[](https://www.youtube.com/watch?v=EwlKwbyK8GI)

### Haar Features und Gesichtserkennung

    --{{0}}--
Ein weiterer klassischer Algorithmus f√ºr Objekterkennung sind Haar-artige Features, die durch Viola und Jones 2001 bekannt wurden. Diese Methode war bahnbrechend f√ºr die Echtzeit-Gesichtserkennung und wird in OpenCV als Haar Cascade Classifier implementiert.

**Grundidee der Haar Features:**

Haar Features basieren auf der Berechnung von Helligkeitsunterschieden zwischen benachbarten Rechteckbereichen im Bild:

```ascii
Haar Feature Typen:

  Edge Features:          Line Features:         Four-Rectangle:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ     ‚îÇ          ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ     ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ     ‚îÇ
  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ     ‚îÇ          ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ     ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ     ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ
                                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  Vertikal   Horizontal

  Feature-Wert = Œ£(wei√üe Pixel) - Œ£(schwarze Pixel)                                                      .
```

Diese einfachen Features k√∂nnen komplexe Strukturen erfassen:

+ **Kantenfeatures**: Erkennen √úberg√§nge zwischen hellen und dunklen Bereichen
+ **Linienfeatures**: Erkennen dunkle Linien auf hellem Hintergrund (z.B. Augenbrauen)
+ **Rechteckfeatures**: Erkennen kontrastierende Bereiche (z.B. Nase heller als Augenpartie)

Parameter pro Feature in einer Stage sind z.B.:

+ Position (x, y) im Bildfenster
+ Gr√∂√üe (Breite, H√∂he) der Rechtecke
+ Art des Features (Edge horizontal, Edge vertikal, Line etc.)
+ Gewicht (wie wichtig dieses Feature ist)
+ Schwellenwerte (Thresholds) zur Entscheidungsfindung

**Integralbilder f√ºr schnelle Berechnung:**

Der Trick f√ºr Echtzeit-Performance liegt in der Verwendung von Integralbildern:

$$II(x,y) = \sum_{x' \leq x, y' \leq y} I(x', y')$$

Mit dem Integralbild kann die Summe jedes Rechtecks in **konstanter Zeit O(1)** berechnet werden:

```ascii
Rechteck-Summe mit 4 Array-Zugriffen:

     A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ B
     ‚îÇ           ‚îÇ
     ‚îÇ  Rechteck ‚îÇ     Summe = II(D) - II(B) - II(C) + II(A)
     ‚îÇ           ‚îÇ
     C ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ D                                                                                     .
```

**AdaBoost Cascade Classifier:**

Das Training verwendet AdaBoost, um aus tausenden m√∂glicher Haar Features die relevantesten auszuw√§hlen und in einer Kaskade anzuordnen:

```ascii
Cascade Structure:

  Bild-       Stage 1        Stage 2        Stage 3        Face
  Region  ‚îÄ‚îÄ‚îÄ (wenige    ‚îÄ‚îÄ‚îÄ (mehr     ‚îÄ‚îÄ‚îÄ (viele    ‚îÄ‚îÄ‚îÄ Detected!
               Features)      Features)      Features)

     ‚îÇ            ‚îÇ              ‚îÇ              ‚îÇ
     ‚ñº            ‚ñº              ‚ñº              ‚ñº
  Reject      Reject         Reject         Reject
  (schnell)   (schnell)      (langsam)      (langsam)                                                   .
```

+ Fr√ºhe Stufen haben wenige Features ‚Üí schnelle Ablehnung von Nicht-Gesichtern
+ Sp√§tere Stufen werden nur f√ºr vielversprechende Kandidaten ausgef√ºhrt
+ ~95% der Bildregionen werden bereits in den ersten Stufen verworfen

**OpenCV Implementierung:**

```python
import cv2

# Lade vortrainierten Haar Cascade Classifier
face_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
)

# Bild laden und in Graustufen konvertieren
img = cv2.imread('gruppe.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Gesichtserkennung durchf√ºhren
faces = face_cascade.detectMultiScale(
    gray,
    scaleFactor=1.1,    # Skalierungsfaktor zwischen Scans
    minNeighbors=5,     # Mindestanzahl benachbarter Detektionen
    minSize=(30, 30)    # Minimale Gesichtsgr√∂√üe
)

# Ergebnisse visualisieren
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)

print(f"Gefundene Gesichter: {len(faces)}")
cv2.imwrite('detected_faces.jpg', img)
```

**Parameter erkl√§rt:**

| Parameter | Bedeutung | Typischer Wert |
|-----------|-----------|----------------|
| `scaleFactor` | Verkleinerungsfaktor pro Scan-Durchlauf | 1.05 - 1.3 |
| `minNeighbors` | Erforderliche √úberlappungen f√ºr Detektion | 3 - 6 |
| `minSize` | Minimale Objektgr√∂√üe in Pixeln | (30, 30) |
| `maxSize` | Maximale Objektgr√∂√üe in Pixeln | unbegrenzt |

**Weitere verf√ºgbare Cascade Classifier in OpenCV:**

+ `haarcascade_eye.xml` - Augenerkennung
+ `haarcascade_smile.xml` - L√§cheln-Erkennung
+ `haarcascade_profileface.xml` - Gesichter im Profil
+ `haarcascade_fullbody.xml` - Ganzk√∂rper-Erkennung
+ `haarcascade_upperbody.xml` - Oberk√∂rper

!?[](https://www.youtube.com/watch?v=hPCTwxF0qf4&t=103s)

## Deep Learning f√ºr Objekterkennung

> Deep Learning ist eine Kategorie des maschinellen Lernens, der auf tiefen k√ºnstlichen neuronalen Netzen basiert. ‚ÄûDeep‚Äú bezeichnet die Anzahl hintereinandergeschalteter Verarbeitungsebenen (Layer) in einem neuronalen Netz.

+ Viele hintereinandergeschaltete Schichten
+ Automatische Merkmalsextraktion
+ Hoher Daten- und Rechenbedarf
+ Besonders erfolgreich bei unstrukturierten Daten

```ascii
K√ºnstliche Intelligenz
 ‚îî‚îÄ Maschinelles Lernen
     ‚îî‚îÄ Deep Learning                                                   .
```

> ‚Äû‚ÄöDeep‚Äò bezeichnet die Tiefe der Repr√§sentationshierarchie: Viele aufeinanderfolgende nichtlineare Transformationen, die aus Rohdaten schrittweise abstrakte Merkmale formen.‚Äú

| Aspekt          | Haar Cascade                      | Deep Learning (YOLO)    |
| --------------- | --------------------------------- | ----------------------- |
| Geschwindigkeit | Sehr schnell (CPU)                | Schnell (ben√∂tigt GPU)  |
| Genauigkeit     | Gut f√ºr frontale Gesichter        | Besser bei Variationen  |
| Robustheit      | Empfindlich auf Rotation          | Robust                  |
| Training        | Aufwendig, ben√∂tigt viele Samples | End-to-End Training     |
| Speicherbedarf  | Sehr gering (`~1 MB`)             | Gr√∂√üer (`~10-100 MB`)     |
| Anwendungsfall  | Embedded Systems, Echtzeit        | Server, komplexe Szenen |

> **Fazit:** Haar Cascade Classifier sind ein gutes Beispiel daf√ºr, wie mit cleveren mathematischen Tricks (Integralbilder) und Machine Learning (AdaBoost) effiziente Objekterkennung m√∂glich ist. F√ºr viele Embedded-Anwendungen sind sie aufgrund ihrer Geschwindigkeit und geringen Ressourcenanforderungen immer noch relevant.

### DL Architekturen

> CNNs sind nicht das einzige Werkzeug des Deep Learning!

| Typ                                     | Typische Anwendungen                 | Beispiele f√ºr Anwendungen                                                                                                             |
| --------------------------------------- | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |
| **CNN (Convolutional Neural Networks)** | Bilder, Videos, r√§umliche Daten      | Objekterkennung (z.B. YOLO, Gesichtserkennung), Bildklassifikation (z.B. Katzen vs. Hunde), medizinische Bildanalyse (Tumorerkennung) |
| **RNN (Recurrent Neural Networks)**     | Zeitreihen, Sprache, Sequenzen       | Handschriftenerkennung, maschinelle √úbersetzung, Sprachgenerierung, Aktienkursvorhersage                                              |
| **Transformers**                        | Sprache (NLP), Bild, multimodal      | Chatbots (z.B. ChatGPT), maschinelle √úbersetzung, Textzusammenfassung, Bildunterschriften-Generierung                                 |
| **Fully Connected / Dense Nets**        | Klassifikation, einfache ML-Aufgaben | Kreditw√ºrdigkeitspr√ºfung, Spam-Erkennung, Iris-Blumenklassifikation (ein klassisches ML-Beispiel)                                     |
| **Autoencoder, GANs, etc.**             | Datenkompression, Generierung        | Bildrauschen entfernen, Bilderzeugung (DeepFakes, Stiltransfer), Anomalieerkennung in Produktionsdaten                                |

### CNN-Grundlagen (Kurzer Einstieg)

**Convolutional Neural Network (CNN):**

```ascii
Input Image          Conv Layer        Pooling         FC Layer      Output
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ     ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇ     ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇ     ‚îÇ ‚îÄ‚îÄ‚îÄ>  ‚îÇCat  ‚îÇ
  ‚îÇ üê±  ‚îÇ  Filter   ‚îÇ ‚ñì‚ñì‚ñì ‚îÇ  MaxPool ‚îÇ ‚ñì ‚îÇ Flatten  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ       ‚îÇDog  ‚îÇ
  ‚îÇ     ‚îÇ           ‚îÇ ‚ñì‚ñì‚ñì ‚îÇ          ‚îÇ ‚ñì ‚îÇ          ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ       ‚îÇBird ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  224√ó224√ó3         112√ó112√ó64       56√ó56√ó64       4096          Classes                                  .
```

**Input Image (Eingabebild):**
Ein Farbbild mit z.B. **224√ó224 Pixeln** und **3 Farbkan√§len** (Rot, Gr√ºn, Blau).
Das Bild wird als 3D-Array betrachtet: H√∂he √ó Breite √ó Farbkan√§le.

**Convolutional Layer (Faltungsschicht):**
Viele kleine Filter (z.B. 3√ó3) werden √ºber das Bild geschoben.

* Jeder Filter erkennt lokale Merkmale wie Kanten, Ecken, Farben oder Texturen.
* Filterwerte werden **w√§hrend des Trainings gelernt** und sind damit flexibel.
* Das Ergebnis sind mehrere **Feature Maps** (hier z.B. 64), die anzeigen, wo im Bild bestimmte Muster auftreten.

*Analogie:* Eine Schablone, die verschiedene Muster √ºber das Bild legt und pr√ºft, wo sie am besten passen.

**Pooling Layer (Pooling-Schicht):**
Reduziert die r√§umlichen Dimensionen der Feature Maps (hier von 112√ó112 auf 56√ó56).

* Typisch ist **Max-Pooling**, bei dem in kleinen Bereichen (z.B. 2√ó2) der gr√∂√üte Wert √ºbernommen wird.
* Dadurch bleibt die wichtigste Information erhalten, w√§hrend die Darstellung kompakter wird.
* Pooling macht das Modell robuster gegen√ºber kleinen Verschiebungen und reduziert Rechenaufwand.

*Analogie:* Eine Zusammenfassung oder Vergr√∂berung, die das Bild kompakter und √ºbersichtlicher macht.

**ReLU Activation (Aktivierungsfunktion) zwischen den Schichten:**
F√ºhrt eine einfache nichtlineare Transformation durch: $f(x) = \max(0, x)$

Diese Nichtlinearit√§t erm√∂glicht es dem Netzwerk, komplexe Muster zu lernen.

*Hinweis:* ReLU ist meist **kein eigener Layer**, sondern wird direkt nach jeder Conv- oder FC-Schicht angewendet.

**Fully Connected Layer (Vollst√§ndig verbundene Schicht):**
Am Ende des Netzes werden alle extrahierten Merkmale **flachgedr√ºckt (flattened)** und als Vektor in den FC-Layer eingespeist.

* Jeder Eingang ist mit jedem Neuron verbunden.
* Der FC-Layer kombiniert alle Merkmale und entscheidet, zu welcher Klasse (z.B. Katze, Hund, Vogel) das Bild geh√∂rt.

> Was bedeutet ‚ÄûDeep‚Äú genau in deinem CNN-Diagramm?

‚ÄûDeep‚Äú bedeutet, dass viele Convolutional Layers (plus weitere Schichten wie Pooling und Aktivierung) hintereinander geschaltet werden, um eine Hierarchie von Merkmalen zu lernen ‚Äî von einfachen Kanten bis zu komplexen Objekten.
Diese Tiefe erm√∂glicht es dem Netzwerk, sehr komplexe Muster und Zusammenh√§nge in den Daten zu erfassen.

### YOLO: You Only Look Once

YOLO ist ein **Single-Shot Detector**: Das gesamte Bild wird in einem einzigen Durchgang verarbeitet, und alle Objekte werden gleichzeitig erkannt. Das macht YOLO extrem schnell!

**YOLO-Prinzip:**

1. **Teile das Bild in ein Grid** (z.B. 13√ó13 Zellen)
2. **Jede Grid-Zelle sagt vorher:**

   * $B$ Bounding Boxes mit Koordinaten ((x, y, w, h)) und Confidence-Wert
   * $C$ Klassenwahrscheinlichkeiten
3. **Non-Maximum Suppression (NMS)** entfernt mehrfach erkannte Objekte (√úberlappungen)

```ascii
Grid-basierte Objekterkennung:

  ‚îå‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚îê
  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§        Grid-Zelle (3,2) detektiert:
  ‚îú‚îÄ‚îº‚îÄ‚îº‚ñà‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§        ‚Ä¢ Bounding Box: (x=3.2, y=2.7, w=2.1, h=3.5)
  ‚îú‚îÄ‚îº‚îÄ‚îº‚ñà‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§  ‚îÄ‚îÄ‚îÄ>  ‚Ä¢ Confidence: 0.89
  ‚îú‚îÄ‚îº‚îÄ‚îº‚ñà‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§        ‚Ä¢ Klasse: "person"
  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§
  ‚îî‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îò
   7√ó7 Grid                                                                                               .
```

**Evolution der YOLO-Familie (Auswahl):**

| Version | Jahr | mAP (COCO) | FPS | Besonderheiten                  |
| ------- | ---- | ---------- | --- | ------------------------------- |
| YOLOv1  | 2016 | ~63.4%     | 45  | Erste Single-Shot Detection     |
| YOLOv2  | 2017 | ~78.6%     | 67  | Batch Norm, Anchor Boxes        |
| YOLOv3  | 2018 | ~57.9%     | 45  | Multi-Scale Predictions         |
| YOLOv4  | 2020 | ~65.7%     | 65  | CSPDarknet Backbone             |
| YOLOv5  | 2020 | ~67.3%     | 140 | PyTorch Implementation, einfach |
| YOLOv8  | 2023 | ~70.0%     | 120 | Aktuell empfohlen               |

> **F√ºr √úbung 2 verwenden wir YOLOv8** ‚Äî beste Balance zwischen Performance und Benutzerfreundlichkeit!


### YOLOv8 im Detail

YOLOv8 ist die aktuell empfohlene Version von Ultralytics. Sie bietet eine sehr einfache Python-API und eignet sich gut f√ºr Integration in ROS 2 und andere Anwendungen.

**Architektur-√úberblick:**

```ascii
YOLOv8 Architektur:

Input               Backbone           Neck              Head
640√ó640√ó3          (CSPDarknet)     (PAN-FPN)      (Detection Head)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇ ‚ñì‚ñì‚ñì ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇ ‚ñì‚ñì  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇBBox ‚îÇ
‚îÇ üö∂  ‚îÇ  Feature     ‚îÇ ‚ñì‚ñì‚ñì ‚îÇ  Multi-  ‚îÇ ‚ñì‚ñì  ‚îÇ  Predict ‚îÇClass‚îÇ
‚îÇ     ‚îÇ  Extraction ‚îÇ ‚ñì‚ñì‚ñì ‚îÇ  Scale   ‚îÇ ‚ñì‚ñì  ‚îÇ          ‚îÇConf ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ca. 20 Mio        Fusion          Outputs  
                    Parameter         Layers                                                               .
```

**YOLOv8 Modell-Varianten:**

| Modell  | Parameteranzahl | mAP  | Speed (ms) | Typische Verwendung            |
| ------- | --------------- | ---- | ---------- | ------------------------------ |
| YOLOv8n | 3.2 Mio         | 37.3 | 1.2        | Embedded, Edge-Devices         |
| YOLOv8s | 11.2 Mio        | 44.9 | 1.9        | Gute Balance, schneller Laptop |
| YOLOv8m | 25.9 Mio        | 50.2 | 3.2        | Standardwahl                   |
| YOLOv8l | 43.7 Mio        | 52.9 | 4.5        | H√∂here Genauigkeit             |
| YOLOv8x | 68.2 Mio        | 53.9 | 6.8        | Beste Genauigkeit              |

> **Empfehlung f√ºr √úbung 2:** YOLOv8n oder YOLOv8s ‚Äî schnell genug f√ºr Echtzeit auf Laptops!

!?[](https://www.youtube.com/watch?v=svn9-xV7wjk)

### YOLOv8 im Detail

YOLOv8 ist die aktuell empfohlene Version von Ultralytics. Sie bietet eine sehr einfache Python-API und ist optimal f√ºr ROS 2 Integration geeignet.

**Installation und Nutzung:**

```python
# Installation
pip install ultralytics

# Import
from ultralytics import YOLO

# Vortrainiertes Modell laden (COCO-Dataset)
model = YOLO('yolov8n.pt')  # nano model

# Inferenz auf einem Bild
results = model('image.jpg')

# Oder auf Video/Webcam
results = model('video.mp4', stream=True)
```

**Output-Format verstehen:**

```python
# Ergebnisse iterieren
for result in results:
    boxes = result.boxes  # Bounding Boxes

    for box in boxes:
        # Koordinaten
        x1, y1, x2, y2 = box.xyxy[0]  # [x_min, y_min, x_max, y_max]

        # Oder: Center + Width/Height
        x_center, y_center, w, h = box.xywh[0]

        # Confidence Score
        confidence = box.conf[0]

        # Klassen-ID (COCO)
        class_id = int(box.cls[0])

        # Klassen-Name
        class_name = model.names[class_id]

        print(f"{class_name}: {confidence:.2f} at ({x1}, {y1})")
```

### COCO-Dataset und Personendetektion

    --{{0}}--
YOLOv8 wird standardm√§√üig auf dem COCO-Dataset trainiert. Dieses enth√§lt 80 Objektklassen.

**COCO (Common Objects in Context):**

+ 80 Objektklassen
+ 330K Bilder
+ 1.5M Objektinstanzen
+ "Industry-Standard" f√ºr Object Detection

https://cocodataset.org/#home

**Wichtige Klassen (Auswahl):**

| Class ID | Name       | Class ID | Name       |
| -------- | ---------- | -------- | ---------- |
| 0        | **person** | 5        | bus        |
| 1        | bicycle    | 16       | dog        |
| 2        | car        | 17       | cat        |
| 3        | motorcycle | 62       | chair      |
| 4        | airplane   | 73       | book       |

**Personendetektion mit YOLOv8:**

```python
from ultralytics import YOLO

# Modell laden
model = YOLO('yolov8n.pt')

# Bild laden
image = cv2.imread('scene.jpg')

# Inferenz
results = model(image)

# Nur Personen filtern
persons = []
for box in results[0].boxes:
    if int(box.cls[0]) == 0:  # person class
        if box.conf[0] > 0.5:  # confidence threshold
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            persons.append({
                'bbox': [x1, y1, x2, y2],
                'confidence': float(box.conf[0]),
                'center': ((x1+x2)/2, (y1+y2)/2)  # Wichtig f√ºr Stereo!
            })

print(f"Detected {len(persons)} persons")
```

> Experimentieren Sie mit der Demo Implementierung im Ordner /yolo_Example.

### Von Bounding Box zu 3D-Position

    --{{0}}--
Dies ist der entscheidende Schritt f√ºr √úbung 2: Wir kombinieren YOLOv8-Detektionen mit der Disparit√§tskarte aus der Stereo-Kamera!

**Pipeline:**

```ascii
Step 1: Detection        Step 2: Center Point      Step 3: Depth Lookup
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ              ‚îÇ    ‚óè    ‚îÇ              ‚îÇ    ‚óè    ‚îÇ
‚îÇ  ‚îÇ üö∂‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  (cx,cy)‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ  Z=3.2m ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  YOLOv8      ‚îÇ         ‚îÇ  Disparity   ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  Map         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 4: 3D Coordinates
   (X, Y, Z) in camera frame
        ‚îÇ
        v
   Transform to robot frame (TF2)                                                                         .
```

**Mathematik:**

> Aus L06 wissen wir: Die Tiefe (Entfernung) eines Punkts zur Kamera ist proportional zum Produkt aus Brennweite und Abstand zwischen den Kameras und umgekehrt proportional zur Disparit√§t (dem Pixelversatz) dieses Punkts in den Stereo-Bildern.

$$
Z = \frac{f \cdot b}{d}
$$

Wobei:

+ $f$ ist die Brennweite der Kamera (in Pixeln).
+ $b$ ist die Baseline, also der Abstand zwischen den beiden Kameras.
+ $d$ ist die Disparit√§t, also die Differenz der Position des Punkts in den zwei Bildern.

Nun nutzt du $Z$ und die Bildkoordinaten $(u, v)$, um die reale Position im Kamerakoordinatensystem zu berechnen:

$$
X = \frac{(u - c_x) \cdot Z}{f_x}
$$

$$
Y = \frac{(v - c_y) \cdot Z}{f_y}
$$

mit 

+ $c_x, c_y$ sind die Koordinaten des Hauptpunkts (Principal Point) in Pixeln
+ $f_x, f_y$ sind die Brennweiten in Pixeln entlang der $x$- bzw. $y$-Achse (aus der Kamerakalibrierung)

**Python-Implementierung:**

```python
def get_3d_position(bbox, disparity_map, camera_info):
    """
    Berechne 3D-Position aus Bounding Box und Disparit√§tskarte

    Args:
        bbox: [x1, y1, x2, y2] in Pixel
        disparity_map: Disparit√§tskarte (gleiche Aufl√∂sung wie Bild)
        camera_info: Kamera-Kalibrierung (fx, fy, cx, cy, baseline)

    Returns:
        (X, Y, Z) in Metern (camera frame)
    """
    # Center der Bounding Box
    cx = int((bbox[0] + bbox[2]) / 2)
    cy = int((bbox[1] + bbox[3]) / 2)

    # Disparit√§t an diesem Punkt
    disparity = disparity_map[cy, cx]

    # Tiefe berechnen (aus L06)
    if disparity > 0:
        Z = (camera_info['fx'] * camera_info['baseline']) / disparity
    else:
        return None  # Keine g√ºltige Tiefe

    # 3D-Koordinaten
    X = (cx - camera_info['cx']) * Z / camera_info['fx']
    Y = (cy - camera_info['cy']) * Z / camera_info['fy']

    return (X, Y, Z)
```

**Robustheit verbessern:**

Problem: Disparit√§t kann Rauschen enthalten oder ung√ºltig sein

L√∂sungen:

```python
# 1. Verwende Median √ºber mehrere Pixel
window_size = 5
cx_min = max(0, cx - window_size//2)
cx_max = min(disparity_map.shape[1], cx + window_size//2)
cy_min = max(0, cy - window_size//2)
cy_max = min(disparity_map.shape[0], cy + window_size//2)

window = disparity_map[cy_min:cy_max, cx_min:cx_max]
disparity = np.median(window[window > 0])  # Ignoriere 0-Werte

# 2. Plausibilit√§ts-Check
if 0.5 < Z < 10.0:  # Personen zwischen 0.5m und 10m
    return (X, Y, Z)
else:
    return None
```

## Object Tracking

    --{{0}}--
Objekterkennung liefert uns Detektionen in einzelnen Bildern. Tracking verbindet diese √ºber Zeit und erm√∂glicht uns, Bewegungen vorherzusagen und IDs zu erhalten.

**Vorteile gegen√ºber reiner Detektion:**

+ **Temporal Coherence**: Gl√§ttung √ºber Zeit reduziert Rauschen
+ **ID-Konsistenz**: "Person #1" bleibt "Person #1" √ºber mehrere Frames
+ **Pr√§diktive Suche**: Effizienter durch Einschr√§nkung des Suchbereichs
+ **Okklusion-Handling**: Objekte k√∂nnen kurzzeitig verdeckt werden
+ **Trajektorien**: Geschwindigkeit und Richtung bestimmen

**Anwendungen:**

+ Autonome Fahrzeuge: Fu√üg√§nger-Trajektorien vorhersagen
+ Person-Following Roboter: Verfolge spezifische Person
+ Videoanalyse: Z√§hle Personen, die durch T√ºr gehen

### Optical Flow: Lucas-Kanade

    --{{0}}--
Optischer Fluss beschreibt die Bewegung von Pixeln zwischen zwei aufeinanderfolgenden Bildern.

**Annahmen:**

1. **Brightness Constancy**: Pixel-Intensit√§t bleibt konstant
   $$I(x, y, t) = I(x + \Delta x, y + \Delta y, t + \Delta t)$$

2. **Small Motion**: Bewegung ist klein zwischen Frames

3. **Spatial Coherence**: Nachbar-Pixel bewegen sich √§hnlich

**Grundannahme der Lucas-Kanade Methode:** Ein Pixel beh√§lt seine Helligkeit zwischen den Frames: $I(x, y, t) = I(x + u, y + v, t + 1)$

**Realisierung:**

| Gradient | Berechnung | Bedeutung |
|----------|------------|-----------|
| $I_x$ | Sobel in x-Richtung | Helligkeits√§nderung horizontal |
| $I_y$ | Sobel in y-Richtung | Helligkeits√§nderung vertikal |
| $I_t$ | Frame2 - Frame1 | Helligkeits√§nderung zeitlich |

Die Verschiebungen werden auf Basis der **Helligkeitsgradienten** berechnet - nicht durch Deskriptor-Matching wie bei ORB/SIFT.

```ascii 
Fenster um Feature-Punkt (z.B. 5√ó5 Pixel):

Frame t:              Frame t+1:           Differenz (I_t):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 120 125 130 ‚îÇ       ‚îÇ 118 123 128 ‚îÇ      ‚îÇ -2  -2  -2  ‚îÇ
‚îÇ 140 200 145 ‚îÇ  ‚Üí    ‚îÇ 142 202 147 ‚îÇ  =   ‚îÇ +2  +2  +2  ‚îÇ
‚îÇ 135 130 125 ‚îÇ       ‚îÇ 137 132 127 ‚îÇ      ‚îÇ +2  +2  +2  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                  .
``` 

Aus den r√§umlichen Gradienten ($I_x$, $I_y$) und der zeitlichen √Ñnderung ($I_t$) wird ein **√ºberbestimmtes Gleichungssystem** aufgestellt (eine Gleichung pro Pixel im Fenster):

**Beispiel f√ºr 2 Pixel:**

Pixel 1 an Position $(x_1, y_1)$: $\quad I_{x,1} \cdot u + I_{y,1} \cdot v = -I_{t,1}$

Pixel 2 an Position $(x_2, y_2)$: $\quad I_{x,2} \cdot u + I_{y,2} \cdot v = -I_{t,2}$

In Matrixform:

$$
\begin{bmatrix} I_{x,1} & I_{y,1} \\ I_{x,2} & I_{y,2} \end{bmatrix}
\begin{bmatrix} u \\ v \end{bmatrix} =
\begin{bmatrix} -I_{t,1} \\ -I_{t,2} \end{bmatrix}
$$

Bei einem 5√ó5 Fenster haben wir **25 Gleichungen** f√ºr nur **2 Unbekannte** $(u, v)$ ‚Üí √ºberbestimmt! Die L√∂sung erfolgt per **Least Squares**.

**Unterschied zu Feature-Matching:**

| Lucas-Kanade | ORB/SIFT Matching |
|--------------|-------------------|
| Nutzt **Gradienten** (Helligkeits√§nderungen) | Nutzt **Deskriptoren** (Bit-Strings) |
| Sucht minimalen Helligkeitsfehler | Sucht minimale Hamming-Distanz |
| Braucht **kleine Bewegungen** | Funktioniert bei gro√üen Verschiebungen |
| Sehr schnell (nur Matrixinversion) | Aufwendiger (alle Paare vergleichen) |

> Die Verschiebung wird also **nicht durch Suche nach √§hnlichen Mustern** gefunden, sondern durch **L√∂sen einer Gleichung**, die beschreibt, wohin sich der Helligkeitsgradient bewegt haben muss.

### DeepSORT: Deep Simple Online Realtime Tracking

    --{{0}}--
DeepSORT ist der aktuelle Standard f√ºr Multi-Object Tracking. Er kombiniert Kalman-Filter mit Deep Learning Features.

**DeepSORT-Pipeline:**

```ascii
Frame t-1                     Frame t
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Person1 ‚îÇ                  ‚îÇ Person? ‚îÇ
‚îÇ Person2 ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ Person? ‚îÇ
‚îÇ Person3 ‚îÇ  Predict & Match ‚îÇ Person? ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                             ‚îÇ
    v                             v
Kalman Filter Prediction    YOLO Detections
    ‚îÇ                             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               v
        Hungarian Algorithm
        (Data Association)
               ‚îÇ
               v
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Track 1 ‚îÇ ‚Üê Person1 matched
        ‚îÇ Track 2 ‚îÇ ‚Üê Person2 matched
        ‚îÇ Track 3 ‚îÇ ‚Üê Person3 lost (tentative)
        ‚îÇ Track 4 ‚îÇ ‚Üê New detection
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                                                    .
```

**Komponenten:**

**1. Detection (YOLO)**
   - Liefert Bounding Boxes pro Frame

**2. Kalman Filter**
   - Pr√§diziert Position im n√§chsten Frame
   - State: $(x, y, a, h, \dot{x}, \dot{y}, \dot{a}, \dot{h})$
     - $(x, y)$ = Center
     - $a$ = Aspect Ratio
     - $h$ = H√∂he
     - Ableitungen = Geschwindigkeiten

**3. Appearance Descriptor**
   - CNN-Features f√ºr jede Detection
   - Hilft bei Re-Identification nach Okklusion
   - Typisch: 128-dim Feature Vector

**4. Hungarian Algorithm**
   - Optimale Zuordnung: Detection ‚Üí Track
   - Minimiert kombinierte Kostenfunktion:
     $$c_{i,j} = \lambda \cdot d_{\text{Mahalanobis}} + (1-\lambda) \cdot d_{\text{Cosine}}$$

**5. Track Management**
   - **Confirmed**: Track existiert √ºber $n$ Frames
   - **Tentative**: Neue Detection, noch unsicher
   - **Deleted**: Track verloren √ºber $m$ Frames

**Kostenfunktion im Detail:**

$$
d_{\text{Mahalanobis}} = \sqrt{(d_j - y_i)^T S_i^{-1} (d_j - y_i)}
$$

+ $d_j$ = Detection $j$
+ $y_i$ = Kalman-Pr√§diktion f√ºr Track $i$
+ $S_i$ = Kovarianzmatrix

$$
d_{\text{Cosine}} = 1 - \frac{r_j^T \cdot r_i^k}{||r_j|| \cdot ||r_i^k||}
$$

+ $r_j$ = Appearance Feature von Detection $j$
+ $r_i^k$ = Appearance Features von Track $i$ (letzten $k$ Frames)


## 3D-Objekterkennung in Punktwolken

Bisher haben wir 2D-Bilder betrachtet. F√ºr mobile Roboter sind Punktwolken aus Lidar oder Stereo-Kameras ebenso wichtig. Hier lernen wir grundlegende 3D-Algorithmen.

### RANSAC: Plane Segmentation

    --{{0}}--
RANSAC (Random Sample Consensus) ist ein robuster Algorithmus zum Fitten von Modellen in verrauschten Daten.

**Problem**: Finde Boden-Ebene in Punktwolke

**Ebenen-Gleichung:**

$$
ax + by + cz + d = 0
$$

Normalisiert mit $\sqrt{a^2 + b^2 + c^2} = 1$

**RANSAC-Algorithmus:**

F√ºr k Iterationen:

  1. W√§hle zuf√§llig 3 Punkte
  2. Berechne Ebene durch diese Punkte
  3. Z√§hle Inliers (Punkte mit Distanz < threshold)
  4. Speichere beste L√∂sung

R√ºckgabe: Ebene mit meisten Inliers 

RANSAC ist keine klassische kontinuierliche Optimierung (z.B. Gradient-Descent auf einer Differenzierbaren Kostenfunktion), sondern:

+ Stochastisch: Es probiert viele zuf√§llige Modell-Kandidaten aus.
+ Diskret: Bewertet Modelle nach Anzahl der Inlier (eine diskrete, nicht differenzierbare "Kostenfunktion").

**Mathematik:**

Ebene durch 3 Punkte $P_1, P_2, P_3$:

Normalenvektor:
$$
\vec{n} = (P_2 - P_1) \times (P_3 - P_1)
$$

Ebenengleichung:
$$
\vec{n} \cdot (P - P_1) = 0
$$

Distanz Punkt $P$ zur Ebene:
$$
d = \frac{|ax_P + by_P + cz_P + d|}{\sqrt{a^2 + b^2 + c^2}}
$$

Oft wird das beste Modell noch durch eine Least-Squares-Optimierung auf den gefundenen Inliern feinjustiert ‚Äî das ist dann eine klassische kontinuierliche Optimierung, die Fehler quadratisch minimiert.

**Anwendungen:**

+ Boden-Entfernung f√ºr Navigation
+ Wand-Detektion
+ Tisch-Oberfl√§chen finden (f√ºr Grasping)

### Euclidean Clustering

    --{{0}}--
Nach dem Entfernen des Bodens wollen wir einzelne Objekte separieren - das macht Euclidean Clustering.

**Prinzip:**

```ascii
Punktwolke nach Boden-Entfernung:

  ¬∑¬∑  ¬∑¬∑    ¬∑¬∑¬∑         Cluster 1  Cluster 2  Cluster 3
  ¬∑¬∑  ¬∑¬∑    ¬∑¬∑¬∑    ‚Üí      ‚ñà‚ñà         ‚ñà‚ñà         ‚ñà‚ñà‚ñà
 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  (Boden entfernt)              (Boden)                                                                 .
```

**Algorithmus (DBSCAN-√§hnlich):**

```
1. Erstelle KD-Tree f√ºr schnelle Nachbarsuche
2. F√ºr jeden Punkt p:
   - Wenn p schon zugeordnet: Skip
   - Erstelle neues Cluster
   - Finde alle Nachbarn in Radius r
   - F√ºge Nachbarn rekursiv hinzu
3. Filtere Cluster nach Gr√∂√üe (min/max Punkte)
```

### 3D Feature Descriptors: FPFH

    --{{0}}--
Um Objekte in 3D zu erkennen, brauchen wir Deskriptoren - √§hnlich wie ORB in 2D.

**FPFH (Fast Point Feature Histograms)**

Beschreibt die lokale Geometrie um einen Punkt herum.

**Berechnung:**

```ascii
Schritt 1: Normalen          Schritt 2: SPFH             Schritt 3: FPFH
                             (f√ºr Punkt p)               (Aggregation)

    ‚Üó n‚ÇÅ                         p                           p
   ¬∑                            /|\                         /|\
  ‚Üó n‚ÇÇ                         / | \                    ‚îÄ‚îÄ‚îÄ/‚îÄ|‚îÄ\‚îÄ‚îÄ‚îÄ
 ¬∑    ¬∑                      q‚ÇÅ  q‚ÇÇ  q‚ÇÉ                  gewichtete
    ‚Üó n‚ÇÉ                     Winkel Œ±,œÜ,Œ∏               Summe der
   ¬∑                         ‚Üí Histogramm                Nachbar-SPFHs                                      .
```

**Schritt 1: Surface Normals berechnen**

F√ºr jeden Punkt wird die lokale Oberfl√§chenorientierung gesch√§tzt. Dazu wird eine Ebene durch die Nachbarpunkte gefittet (PCA oder Least Squares) - der Normalenvektor dieser Ebene ist die Surface Normal.

**Schritt 2: SPFH (Simplified Point Feature Histogram)**

F√ºr einen Punkt $p$ betrachten wir **alle** Nachbarn $q_1, q_2, ..., q_k$ im Radius $r$ (typisch: 20-50 Nachbarn, nicht nur 3!). F√ºr **jedes Paar** $(p, q_i)$ berechnen wir drei Winkel.

**Warum drei Winkel?** Die relative Lage zweier orientierter Punkte im 3D-Raum hat mehrere Freiheitsgrade:

Ein Winkel allein w√ºrde z.B. nicht unterscheiden, ob der Nachbar "links" oder "rechts" von $p$ liegt, oder ob die Normale "nach vorne" oder "zur Seite" zeigt. Die drei Winkel kodieren die vollst√§ndige relative Orientierung in einem lokalen Koordinatensystem $(u, v, w)$, das an $p$ verankert ist.

```ascii
Beispiel: Punkt p hat 4 Nachbarn im Radius r

         q‚ÇÇ
          ¬∑
    q‚ÇÅ ¬∑  p  ¬∑ q‚ÇÉ        F√ºr jedes Paar (p,q) berechne Œ±, œÜ, Œ∏:
          ¬∑                 (p,q‚ÇÅ) ‚Üí Œ±=0.2,  œÜ=0.8,  Œ∏=1.5
         q‚ÇÑ                 (p,q‚ÇÇ) ‚Üí Œ±=0.7,  œÜ=0.3,  Œ∏=0.9
                            (p,q‚ÇÉ) ‚Üí Œ±=0.3,  œÜ=0.6,  Œ∏=2.1
                            (p,q‚ÇÑ) ‚Üí Œ±=0.5,  œÜ=0.4,  Œ∏=1.2                                                 .
```

Diese Winkelwerte werden nun in **drei separate Histogramme** einsortiert:

```ascii
Histogramm f√ºr Œ±:          Histogramm f√ºr œÜ:          Histogramm f√ºr Œ∏:
(Wertebereich z.B. -1..1)  (Wertebereich 0..1)        (Wertebereich 0..2œÄ)

H√§ufigkeit                 H√§ufigkeit                 H√§ufigkeit
    ‚îÇ   ‚ñì                      ‚îÇ ‚ñì                        ‚îÇ     ‚ñì
    ‚îÇ   ‚ñì ‚ñì                    ‚îÇ ‚ñì ‚ñì                      ‚îÇ ‚ñì   ‚ñì
    ‚îÇ ‚ñì ‚ñì ‚ñì                    ‚îÇ ‚ñì ‚ñì ‚ñì                    ‚îÇ ‚ñì ‚ñì ‚ñì
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Bins          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Bins          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Bins
     1 2 3 ... 11               1 2 3 ... 11               1 2 3 ... 11                                   .
```

Die drei Histogramme werden konkateniert ‚Üí **33 Werte** (3 √ó 11 Bins). Die Bin-Anzahl 11 ist empirisch gew√§hlt (einstellbar in PCL).

**Schritt 3: FPFH (Fast Point Feature Histogram)**

Das "Fast" kommt daher, dass nicht alle Punktpaare betrachtet werden. Stattdessen wird der SPFH von $p$ mit den SPFHs seiner Nachbarn gewichtet kombiniert:

$$FPFH(p) = SPFH(p) + \frac{1}{k} \sum_{i=1}^{k} \frac{1}{d_i} \cdot SPFH(q_i)$$

N√§here Nachbarn ($d_i$ klein) haben mehr Einfluss. So entsteht ein 33-dimensionaler Deskriptor, der die lokale 3D-Geometrie charakterisiert.

**Features:**

+ 33-dimensionaler Histogram-Descriptor
+ Rotation-invariant
+ Robust gegen Rauschen
+ Skaliert gut (schneller als PFH)

**Anwendung: Object Recognition**

```cpp
// Template Matching mit FPFH
// 1. Berechne FPFH f√ºr Template
// 2. Berechne FPFH f√ºr Szene
// 3. Finde korrespondierende Features (KNN)
// 4. RANSAC f√ºr robuste Transformation
// 5. ICP f√ºr Fine-Alignment
```

### ICP: Iterative Closest Point

    --{{0}}--
ICP ist der Standardalgorithmus f√ºr Point Cloud Registration - das Ausrichten zweier Punktwolken.

**Problem**: Gegeben zwei Punktwolken $P$ und $Q$, finde Transformation $T$, sodass $T(P) \approx Q$

**ICP-Algorithmus:**

```
Initialisierung: T = I (Identit√§t)

Wiederhole bis Konvergenz:
  1. F√ºr jeden Punkt p in P:
     - Finde n√§chsten Punkt q in Q

  2. Berechne optimale Transformation T':
     - Minimiere Sum of Squared Distances

  3. Update: T = T' ¬∑ T

  4. Wenn √Ñnderung < threshold: Stop
```

**Mathematik:**

Minimiere:
$$
E(R, t) = \sum_{i=1}^{N} || R p_i + t - q_i ||^2
$$

Wobei:
+ $R$ = Rotationsmatrix (3√ó3)
+ $t$ = Translationsvektor (3√ó1)

L√∂sung: **SVD (Singular Value Decomposition)**

**Anwendungen:**

+ Template Matching (Objekterkennung)
+ SLAM (Scan Matching)
+ 3D-Rekonstruktion
+ Pose Estimation

**Limitierungen:**

+ Ben√∂tigt gute Initialisierung
+ Konvergiert zu lokalem Minimum
+ Langsam bei gro√üen Punktwolken

**Verbesserungen:**

+ **Generalized ICP**: Verwendet Plane-to-Plane statt Point-to-Point
+ **Point-to-Plane ICP**: Verwendet Surface Normals
+ **Feature-based Registration**: Erst grobe Ausrichtung mit Features (FPFH), dann ICP

### Deep Learning f√ºr 3D: PointNet++

    --{{0}}--
Moderne Ans√§tze verwenden neuronale Netze direkt auf Punktwolken - ohne Voxelisierung!

**PointNet (2017):**

Erste End-to-End Deep Learning auf Punktwolken

+ Input: $N \times 3$ Matrix (N Punkte mit x, y, z)
+ Permutation-invariant durch Max-Pooling
+ Klassifikation und Segmentation

**PointNet++ (2017):**

Hierarchische Architektur mit lokalem Kontext

```ascii
Input Points         Set Abstraction      Classification
  N √ó 3               Layers              Output
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ¬∑ ¬∑ ¬∑‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇ ‚ñì‚ñì‚ñì  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇ Car  ‚îÇ
‚îÇ¬∑ ¬∑ ¬∑ ‚îÇ  Group &   ‚îÇ ‚ñì‚ñì   ‚îÇ  MLP +     ‚îÇ Tree ‚îÇ
‚îÇ ¬∑ ¬∑ ¬∑‚îÇ  Sample    ‚îÇ ‚ñì    ‚îÇ  Pooling   ‚îÇ ...  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                          .
```

**Vorteile:**

+ Direkte Verarbeitung von Punktwolken
+ Permutation-invariant
+ Robust gegen unterschiedliche Dichten

**Anwendungen:**

+ 3D Object Classification
+ 3D Object Detection (f√ºr autonome Fahrzeuge)
+ 3D Semantic Segmentation

**Aktuelle Forschung (2024):**

+ **Point Transformer**: Self-Attention f√ºr Punktwolken
+ **VoxelNet**: Hybrid aus Voxeln und PointNet
+ **BEVFusion**: Multi-Modal (Camera + Lidar) f√ºr autonome Fahrzeuge

> Deep Learning f√ºr 3D ist ein aktives Forschungsgebiet - die Methoden entwickeln sich rasant!

## Zusammenfassung & Ausblick

    --{{0}}--
Wir haben heute eine umfassende Einf√ºhrung in Objekterkennung und Tracking erhalten - von klassischen Feature-basierten Methoden bis zu modernem Deep Learning, von 2D-Bildern bis zu 3D-Punktwolken.

### Was haben wir gelernt?

Die Vorlesung spannte einen Bogen von klassischen bis hin zu modernen Verfahren der Objekterkennung:

**Klassische Feature-Methoden** bilden die Grundlage f√ºr geometrische Anwendungen wie SLAM und visuelle Odometrie. Mit Harris und Shi-Tomasi haben wir Corner-Detektoren kennengelernt, die mathematisch fundiert arbeiten und ohne Training auskommen. ORB kombiniert schnelle Detektion (FAST) mit kompakten bin√§ren Deskriptoren (rBRIEF).

**Deep Learning** erm√∂glicht semantisches Verst√§ndnis von Szenen. YOLOv8 erkennt Objekte in Echtzeit und liefert die Bounding Boxes, die wir in √úbung 2 mit Stereo-Tiefendaten kombinieren werden. Die Vortrainierung auf COCO macht den Einstieg einfach - gleichzeitig sollte man die Grenzen dieser "Black Box"-Modelle im Blick behalten.

**Tracking** erweitert die Einzelbild-Detektion um zeitliche Konsistenz. Der Lucas-Kanade Algorithmus verfolgt Features zwischen Frames durch Gradientenanalyse, w√§hrend DeepSORT komplexe Szenen mit mehreren Objekten handhabt und dabei IDs √ºber Verdeckungen hinweg erh√§lt.

**3D-Verfahren** nutzen die Punktwolken aus Lidar oder Stereo-Kameras. RANSAC segmentiert robuste Ebenen (etwa den Boden), Euclidean Clustering separiert Objekte, und ICP aligniert Punktwolken - wichtig f√ºr Scan-Matching in SLAM. Mit PointNet++ haben wir einen Ausblick auf Deep Learning direkt auf 3D-Daten gegeben.


### Ausblick: N√§chste Vorlesung

**L09: Sensordatenfusion I - Grundlagen**

+ Warum mehrere Sensoren kombinieren?
+ Diskrete Bayes-Filter
+ Komplement√§rfilter (IMU-Fusion)
+ Fehlerfortpflanzung und Unsicherheiten
+ Vorbereitung auf Kalman-Filter